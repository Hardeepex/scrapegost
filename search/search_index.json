{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p><code>scrapeghost</code> is an experimental library for scraping websites using OpenAI's GPT APIs.</p> <p>The library provides a means to scrape structured data from HTML without writing page-specific code.</p> <p>Important</p> <p>This library is very experimental with a rapidly evolving interface. No guarantees are made about the stability of the API or the accuracy of the results.</p> <p>Additionally, be aware of the potential costs before using this library.</p> <p>Use at your own risk.</p> <p>Currently licensed under Hippocratic License 3.0.   (See FAQ.)</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Step 1) Obtain an OpenAI API key (https://platform.openai.com) and set an environment variable:</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>Step 2) Install the library however you like:</p> <p><pre><code>pip install scrapeghost\n</code></pre> or <pre><code>poetry add scrapeghost\n</code></pre></p> <p>Step 3) Instantiate a <code>SchemaScraper</code> by defining the shape of the data you wish to extract:</p> <pre><code>from scrapeghost import SchemaScraper\nscrape_legislators = SchemaScraper(\n  schema={\n      \"name\": \"string\",\n      \"url\": \"url\",\n      \"district\": \"string\",\n      \"party\": \"string\",\n      \"photo_url\": \"url\",\n      \"offices\": [{\"name\": \"string\", \"address\": \"string\", \"phone\": \"string\"}],\n  }\n)\n</code></pre> <p>Note</p> <p>There's no pre-defined format for the schema, the GPT models do a good job of figuring out what you want and you can use whatever values you want to provide hints.</p> <p>Step 4) Passing the scraper a URL (or HTML) to the resulting scraper will return a dictionary of the scraped data:</p> <p><pre><code> scrape_legislators(\"https://www.ilga.gov/house/rep.asp?MemberID=3071\")\n</code></pre> <pre><code>{\"name\": \"Emanuel 'Chris' Welch\",\n\"url\": \"https://www.ilga.gov/house/Rep.asp?MemberID=3071\",\n\"district\": \"7th\", \"party\": \"D\", \"photo_url\": \"https://www.ilga.gov/images/members/{5D419B94-66B4-4F3B-86F1-BFF37B3FA55C}.jpg\",\n\"offices\": [\n{\"name\": \"Springfield Office\",\n\"address\": \"300 Capitol Building, Springfield, IL 62706\",\n\"phone\": \"(217) 782-5350\"},\n{\"name\": \"District Office\",\n\"address\": \"10055 W. Roosevelt Rd., Suite E, Westchester, IL 60154\",\n\"phone\": \"(708) 450-1000\"}\n]}\n</code></pre></p> <p>That's it!</p>"},{"location":"#command-line-usage-example","title":"Command Line Usage Example","text":"<p>If you've installed the package (e.g. with <code>pipx</code>), you can use the <code>scrapeghost</code> command line tool to experiment.</p> <pre><code>$ scrapeghost https://www.ncleg.gov/Members/Biography/S/436  \\\n--schema \"{'first_name': 'str', 'last_name': 'str',\n             'photo_url': 'url', 'offices': [] }\"  \\\n--gpt4\n\n{'first_name': 'Gale',\n 'last_name': 'Adcock',\n 'photo_url': 'https://www.ncleg.gov/Members/MemberImage/S/436/Low',\n 'offices': [\n{'address': '16 West Jones Street, Rm. 1104',\n     'city': 'Raleigh', 'state': 'NC', 'zip': '27601',\n     'phone': '(919) 715-3036',\n     'email': 'Gale.Adcock@ncleg.gov',\n     'legislative_assistant': 'Elizabeth Sharpe',\n     'legislative_assistant_email': 'Elizabeth.Sharpe@ncleg.gov'\n}\n]\n}\n</code></pre> <p>See the CLI docs for more details.</p>"},{"location":"#features","title":"Features","text":"<p>The bulk of the work is of course done by the GPT models. The purpose of this library is to provide a convenient interface for using GPT for the purpose of web scraping.</p> <p>Python-based schema definition - Define the shape of the data you want to extract as any Python object.</p> <ul> <li>Future versions will support optional validation that the response matches the schema.</li> </ul> <p>Token Reduction - Fewer tokens means lower costs, faster responses, and staying under the API's token limits.</p> <ul> <li>Automatic HTML cleaning - Remove unnecessary HTML tags and attributes to reduce the size of the HTML sent to the model.</li> <li>CSS and XPath selectors - Pre-filter the HTML to send to the model by writing a single CSS or XPath selector.</li> <li>Auto-splitting - Optionally split the HTML into multiple calls to the model, each of a specified length.</li> </ul> <p>Cost Controls - Scrapers keep running totals of how many tokens have been sent and received, so costs can be tracked.</p> <ul> <li>Future versions will allow setting a budget and stopping the scraper if the budget is exceeded.</li> </ul> <p>Model Options - Works with GPT-3.5-Turbo or GPT 4, and allows passing additional parameters to the model to customize behavior.</p> <ul> <li>Support for automatic fallbacks (e.g. use cost-saving GPT-3.5-Turbo by default, fall back to GPT-4 if needed.)</li> </ul> <p>Error Handling &amp; Logging - Detailed logging and error handling to help debug issues.</p>"},{"location":"LICENSE/","title":"Hippocratic License","text":"<p>HIPPOCRATIC LICENSE</p> <p>Version 3.0, October 2021</p> <p>https://firstdonoharm.dev/version/3/0/extr-ffd-law-mil-sv.md</p> <p>TERMS AND CONDITIONS</p> <p>TERMS AND CONDITIONS FOR USE, COPY, MODIFICATION, PREPARATION OF DERIVATIVE WORK, REPRODUCTION, AND DISTRIBUTION:</p> <p>1. DEFINITIONS:</p> <p>This section defines certain terms used throughout this license agreement.</p> <p>1.1. \u201cLicense\u201d means the terms and conditions, as stated herein, for use, copy, modification, preparation of derivative work, reproduction, and distribution of Software (as defined below).</p> <p>1.2. \u201cLicensor\u201d means the copyright and/or patent owner or entity authorized by the copyright and/or patent owner that is granting the License.</p> <p>1.3. \u201cLicensee\u201d means the individual or entity exercising permissions granted by this License, including the use, copy, modification, preparation of derivative work, reproduction, and distribution of Software (as defined below).</p> <p>1.4. \u201cSoftware\u201d means any copyrighted work, including but not limited to software code, authored by Licensor and made available under this License.</p> <p>1.5. \u201cSupply Chain\u201d means the sequence of processes involved in the production and/or distribution of a commodity, good, or service offered by the Licensee.</p> <p>1.6. \u201cSupply Chain Impacted Party\u201d or \u201cSupply Chain Impacted Parties\u201d means any person(s) directly impacted by any of Licensee\u2019s Supply Chain, including the practices of all persons or entities within the Supply Chain prior to a good or service reaching the Licensee.</p> <p>1.7. \u201cDuty of Care\u201d is defined by its use in tort law, delict law, and/or similar bodies of law closely related to tort and/or delict law, including without limitation, a requirement to act with the watchfulness, attention, caution, and prudence that a reasonable person in the same or similar circumstances would use towards any Supply Chain Impacted Party.</p> <p>1.8. \u201cWorker\u201d is defined to include any and all permanent, temporary, and agency workers, as well as piece-rate, salaried, hourly paid, legal young (minors), part-time, night, and migrant workers.</p> <p>2. INTELLECTUAL PROPERTY GRANTS:</p> <p>This section identifies intellectual property rights granted to a Licensee.</p> <p>2.1. Grant of Copyright License: Subject to the terms and conditions of this License, Licensor hereby grants to Licensee a worldwide, non-exclusive, no-charge, royalty-free copyright license to use, copy, modify, prepare derivative work, reproduce, or distribute the Software, Licensor authored modified software, or other work derived from the Software.</p> <p>2.2. Grant of Patent License: Subject to the terms and conditions of this License, Licensor hereby grants Licensee a worldwide, non-exclusive, no-charge, royalty-free patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer Software.</p> <p>3. ETHICAL STANDARDS:</p> <p>This section lists conditions the Licensee must comply with in order to have rights under this License.</p> <p>The rights granted to the Licensee by this License are expressly made subject to the Licensee\u2019s ongoing compliance with the following conditions:</p> <ul> <li>3.1. The Licensee SHALL NOT, whether directly or indirectly, through agents or assigns:  </li> <li>3.1.1. Infringe upon any person\u2019s right to life or security of person, engage in extrajudicial killings, or commit murder, without lawful cause (See Article 3, United Nations Universal Declaration of Human Rights; Article 6, International Covenant on Civil and Political Rights)  </li> <li>3.1.2. Hold any person in slavery, servitude, or forced labor (See Article 4, United Nations Universal Declaration of Human Rights; Article 8, International Covenant on Civil and Political Rights);  </li> <li>3.1.3. Contribute to the institution of slavery, slave trading, forced labor, or unlawful child labor (See Article 4, United Nations Universal Declaration of Human Rights; Article 8, International Covenant on Civil and Political Rights);  </li> <li>3.1.4. Torture or subject any person to cruel, inhumane, or degrading treatment or punishment (See Article 5, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Civil and Political Rights);  </li> <li>3.1.5. Discriminate on the basis of sex, gender, sexual orientation, race, ethnicity, nationality, religion, caste, age, medical disability or impairment, and/or any other like circumstances (See Article 7, United Nations Universal Declaration of Human Rights; Article 2, International Covenant on Economic, Social and Cultural Rights; Article 26, International Covenant on Civil and Political Rights);  </li> <li>3.1.6. Prevent any person from exercising his/her/their right to seek an effective remedy by a competent court or national tribunal (including domestic judicial systems, international courts, arbitration bodies, and other adjudicating bodies) for actions violating the fundamental rights granted to him/her/them by applicable constitutions, applicable laws, or by this License (See Article 8, United Nations Universal Declaration of Human Rights; Articles 9 and 14, International Covenant on Civil and Political Rights);  </li> <li>3.1.7. Subject any person to arbitrary arrest, detention, or exile (See Article 9, United Nations Universal Declaration of Human Rights; Article 9, International Covenant on Civil and Political Rights);  </li> <li>3.1.8. Subject any person to arbitrary interference with a person\u2019s privacy, family, home, or correspondence without the express written consent of the person (See Article 12, United Nations Universal Declaration of Human Rights; Article 17, International Covenant on Civil and Political Rights);  </li> <li>3.1.9. Arbitrarily deprive any person of his/her/their property (See Article 17, United Nations Universal Declaration of Human Rights);  </li> <li>3.1.10. Forcibly remove indigenous peoples from their lands or territories or take any action with the aim or effect of dispossessing indigenous peoples from their lands, territories, or resources, including without limitation the intellectual property or traditional knowledge of indigenous peoples, without the free, prior, and informed consent of indigenous peoples concerned (See Articles 8 and 10, United Nations Declaration on the Rights of Indigenous Peoples);  </li> <li>3.1.11. Fossil Fuel Divestment: Be an individual or entity, or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, on the FFI Solutions Carbon Underground 200 list;  </li> <li>3.1.12. Extractive Industries: Be an individual or entity, or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, that engages in fossil fuel or mineral exploration, extraction, development, or sale;  </li> <li>3.1.13. Mass Surveillance: Be a government agency or multinational corporation, or a representative, agent, affiliate, successor, attorney, or assign of a government or multinational corporation, which participates in mass surveillance programs;  </li> <li>3.1.14. Military Activities: Be an entity or a representative, agent, affiliate, successor, attorney, or assign of an entity which conducts military activities;  </li> <li>3.1.15. Law Enforcement: Be an individual or entity, or a or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, that provides good or services to, or otherwise enters into any commercial contracts with, any local, state, or federal law enforcement agency;  </li> <li>3.1.16. Interfere with Workers' free exercise of the right to organize and associate (See Article 20, United Nations Universal Declaration of Human Rights; C087 - Freedom of Association and Protection of the Right to Organise Convention, 1948 (No. 87), International Labour Organization; Article 8, International Covenant on Economic, Social and Cultural Rights); and  </li> <li>3.1.17. Harm the environment in a manner inconsistent with local, state, national, or international law.</li> <li>3.2. The Licensee SHALL:  </li> <li>3.2.1. Provide equal pay for equal work where the performance of such work requires equal skill, effort, and responsibility, and which are performed under similar working conditions, except where such payment is made pursuant to:  <ul> <li>3.2.1.1. A seniority system;</li> <li>3.2.1.2. A merit system;  </li> <li>3.2.1.3. A system which measures earnings by quantity or quality of production; or</li> <li>3.2.1.4. A differential based on any other factor other than sex, gender, sexual orientation, race, ethnicity, nationality, religion, caste, age, medical disability or impairment, and/or any other like circumstances (See 29 U.S.C.A. \u00a7 206(d)(1); Article 23, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Economic, Social and Cultural Rights; Article 26, International Covenant on Civil and Political Rights); and  </li> </ul> </li> <li>3.2.2. Allow for reasonable limitation of working hours and periodic holidays with pay (See Article 24, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Economic, Social and Cultural Rights).</li> </ul> <p>4. SUPPLY CHAIN IMPACTED PARTIES:</p> <p>This section identifies additional individuals or entities that a Licensee could harm as a result of violating the Ethical Standards section, the condition that the Licensee must voluntarily accept a Duty of Care for those individuals or entities, and the right to a private right of action that those individuals or entities possess as a result of violations of the Ethical Standards section.</p> <p>4.1. In addition to the above Ethical Standards, Licensee voluntarily accepts a Duty of Care for Supply Chain Impacted Parties of this License, including individuals and communities impacted by violations of the Ethical Standards. The Duty of Care is breached when a provision within the Ethical Standards section is violated by a Licensee, one of its successors or assigns, or by an individual or entity that exists within the Supply Chain prior to a good or service reaching the Licensee.</p> <p>4.2. Breaches of the Duty of Care, as stated within this section, shall create a private right of action, allowing any Supply Chain Impacted Party harmed by the Licensee to take legal action against the Licensee in accordance with applicable negligence laws, whether they be in tort law, delict law, and/or similar bodies of law closely related to tort and/or delict law, regardless if Licensee is directly responsible for the harms suffered by a Supply Chain Impacted Party. Nothing in this section shall be interpreted to include acts committed by individuals outside of the scope of his/her/their employment.</p> <p>5. NOTICE: This section explains when a Licensee must notify others of the License.</p> <p>5.1. Distribution of Notice: Licensee must ensure that everyone who receives a copy of or uses any part of Software from Licensee, with or without changes, also receives the License and the copyright notice included with Software (and if included by the Licensor, patent, trademark, and attribution notice). Licensee must ensure that License is prominently displayed so that any individual or entity seeking to download, copy, use, or otherwise receive any part of Software from Licensee is notified of this License and its terms and conditions. Licensee must cause any modified versions of the Software to carry prominent notices stating that Licensee changed the Software.</p> <p>5.2. Modified Software: Licensee is free to create modifications of the Software and distribute only the modified portion created by Licensee, however, any derivative work stemming from the Software or its code must be distributed pursuant to this License, including this Notice provision.</p> <p>5.3. Recipients as Licensees: Any individual or entity that uses, copies, modifies, reproduces, distributes, or prepares derivative work based upon the Software, all or part of the Software\u2019s code, or a derivative work developed by using the Software, including a portion of its code, is a Licensee as defined above and is subject to the terms and conditions of this License.</p> <p>6. REPRESENTATIONS AND WARRANTIES:</p> <p>6.1. Disclaimer of Warranty: TO THE FULL EXTENT ALLOWED BY LAW, THIS SOFTWARE COMES \u201cAS IS,\u201d WITHOUT ANY WARRANTY, EXPRESS OR IMPLIED, AND LICENSOR SHALL NOT BE LIABLE TO ANY PERSON OR ENTITY FOR ANY DAMAGES OR OTHER LIABILITY ARISING FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THIS LICENSE, UNDER ANY LEGAL CLAIM.</p> <p>6.2. Limitation of Liability: LICENSEE SHALL HOLD LICENSOR HARMLESS AGAINST ANY AND ALL CLAIMS, DEBTS, DUES, LIABILITIES, LIENS, CAUSES OF ACTION, DEMANDS, OBLIGATIONS, DISPUTES, DAMAGES, LOSSES, EXPENSES, ATTORNEYS' FEES, COSTS, LIABILITIES, AND ALL OTHER CLAIMS OF EVERY KIND AND NATURE WHATSOEVER, WHETHER KNOWN OR UNKNOWN, ANTICIPATED OR UNANTICIPATED, FORESEEN OR UNFORESEEN, ACCRUED OR UNACCRUED, DISCLOSED OR UNDISCLOSED, ARISING OUT OF OR RELATING TO LICENSEE\u2019S USE OF THE SOFTWARE. NOTHING IN THIS SECTION SHOULD BE INTERPRETED TO REQUIRE LICENSEE TO INDEMNIFY LICENSOR, NOR REQUIRE LICENSOR TO INDEMNIFY LICENSEE.</p> <p>7. TERMINATION</p> <p>7.1. Violations of Ethical Standards or Breaching Duty of Care: If Licensee violates the Ethical Standards section or Licensee, or any other person or entity within the Supply Chain prior to a good or service reaching the Licensee, breaches its Duty of Care to Supply Chain Impacted Parties, Licensee must remedy the violation or harm caused by Licensee within 30 days of being notified of the violation or harm. If Licensee fails to remedy the violation or harm within 30 days, all rights in the Software granted to Licensee by License will be null and void as between Licensor and Licensee.</p> <p>7.2. Failure of Notice: If any person or entity notifies Licensee in writing that Licensee has not complied with the Notice section of this License, Licensee can keep this License by taking all practical steps to comply within 30 days after the notice of noncompliance. If Licensee does not do so, Licensee\u2019s License (and all rights licensed hereunder) will end immediately.</p> <p>7.3. Judicial Findings: In the event Licensee is found by a civil, criminal, administrative, or other court of competent jurisdiction, or some other adjudicating body with legal authority, to have committed actions which are in violation of the Ethical Standards or Supply Chain Impacted Party sections of this License, all rights granted to Licensee by this License will terminate immediately.</p> <p>7.4. Patent Litigation: If Licensee institutes patent litigation against any entity (including a cross-claim or counterclaim in a suit) alleging that the Software, all or part of the Software\u2019s code, or a derivative work developed using the Software, including a portion of its code, constitutes direct or contributory patent infringement, then any patent license, along with all other rights, granted to Licensee under this License will terminate as of the date such litigation is filed.</p> <p>7.5. Additional Remedies: Termination of the License by failing to remedy harms in no way prevents Licensor or Supply Chain Impacted Party from seeking appropriate remedies at law or in equity.</p> <p>8. MISCELLANEOUS:</p> <p>8.1. Conditions: Sections 3, 4.1, 5.1, 5.2, 7.1, 7.2, 7.3, and 7.4 are conditions of the rights granted to Licensee in the License.</p> <p>8.2. Equitable Relief: Licensor and any Supply Chain Impacted Party shall be entitled to equitable relief, including injunctive relief or specific performance of the terms hereof, in addition to any other remedy to which they are entitled at law or in equity.</p> <p>8.3. Severability: If any term or provision of this License is determined to be invalid, illegal, or unenforceable by a court of competent jurisdiction, any such determination of invalidity, illegality, or unenforceability shall not affect any other term or provision of this License or invalidate or render unenforceable such term or provision in any other jurisdiction. If the determination of invalidity, illegality, or unenforceability by a court of competent jurisdiction pertains to the terms or provisions contained in the Ethical Standards section of this License, all rights in the Software granted to Licensee shall be deemed null and void as between Licensor and Licensee.</p> <p>8.4. Section Titles: Section titles are solely written for organizational purposes and should not be used to interpret the language within each section.</p> <p>8.5. Citations: Citations are solely written to provide context for the source of the provisions in the Ethical Standards.</p> <p>8.6. Section Summaries: Some sections have a brief italicized description which is provided for the sole purpose of briefly describing the section and should not be used to interpret the terms of the License.</p> <p>8.7. Entire License: This is the entire License between the Licensor and Licensee with respect to the claims released herein and that the consideration stated herein is the only consideration or compensation to be paid or exchanged between them for this License. This License cannot be modified or amended except in a writing signed by Licensor and Licensee.</p> <p>8.8. Successors and Assigns: This License shall be binding upon and inure to the benefit of the Licensor\u2019s and Licensee\u2019s respective heirs, successors, and assigns.</p>"},{"location":"_cost/","title":"cost","text":"<p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#schemascraper","title":"<code>SchemaScraper</code>","text":"<p>The <code>SchemaScraper</code> class is the main interface to the API.</p> <p>An <code>SchemaScraper</code> is instantiated with a <code>schema</code> argument, which is a dictionary describing the shape of the data you wish to extract.</p> <ul> <li><code>schema</code> - A dictionary describing the shape of the data you wish to extract.</li> </ul> <p>The following parameters are optional:</p> <ul> <li><code>models</code> - A list of models to use, in order of preference.  Defaults to <code>[\"gpt-3.5-turbo\", \"gpt-4\"]</code>.  </li> <li><code>model_params</code> - A dictionary of parameters to pass to the underlying GPT model.  (See OpenAI docs for details.)</li> <li><code>list_mode</code> - If <code>True</code>, the instructions and behavior will be slightly modified to better perform on pages with lists of similar items.</li> <li><code>extra_instructions</code> - Additional instructions to pass to the GPT model as a system prompt.</li> <li><code>split_length</code> - If set, the scraper will split the page into multiple calls, each of this length. (Only works with <code>list_mode</code>, requires passing a <code>css</code> or <code>xpath</code> selector when scraping.)</li> </ul>"},{"location":"api/#auto-splitting","title":"Auto-splitting","text":"<p>It's worth mentioning how <code>split_length</code> works because it allows for some interesting possibilities but can also become quite expensive.</p> <p>If you pass <code>split_length</code> to the scraper, it assumes the page is made of multiple similar sections and will try to split the page into multiple calls.  </p> <p>When you call the scrape function of an auto-splitting enabled scraper, you are required to pass a <code>css</code> or <code>xpath</code> selector to the function.  The resulting nodes will be combined into chunks no bigger than <code>split_length</code> tokens, sent to the API, and then stitched back together.</p> <p>This seems to work well for long lists of similar items, though whether it is worth the many calls is questionable.</p>"},{"location":"api/#scrape","title":"<code>scrape</code>","text":"<p>The <code>scrape</code> method of a <code>SchemaScraper</code> is used to scrape a page.</p> <pre><code>scraper = SchemaScraper(schema)\nscraper.scrape(\"https://example.com\")\n</code></pre> <ul> <li><code>url_or_html</code> - The first parameter should be a URL or HTML string to scrape.</li> </ul> <p>You can also pass a CSS or XPath selector as a keyword argument:</p> <ul> <li><code>css</code> - A CSS selector to use to filter the HTML before sending it to the API.</li> <li><code>xpath</code> - An XPath selector to use to filter the HTML before sending it to the API.</li> </ul> <p>It is also possible to call the scraper directly, which is equivalent to calling <code>scrape</code>:</p> <pre><code>scraper = SchemaScraper(schema)\nscraper(\"https://example.com\")\n# same as writing\nscraper.scrape(\"https://example.com\")\n</code></pre>"},{"location":"api/#selectors","title":"Selectors","text":"<p>Pass the <code>css</code> or <code>xpath</code> arguments to the scraper to use a selector to narrow down the HTML before sending it to the API.</p> <pre><code>&gt;&gt;&gt; scrape_legislators(\"https://www.ilga.gov/house/rep.asp?MemberID=3071\", xpath=\"//table[1]\")\n</code></pre>"},{"location":"api/#paginatedschemascraper","title":"<code>PaginatedSchemaScraper</code>","text":"<p>TODO: document this</p>"},{"location":"api/#exceptions","title":"Exceptions","text":"<p>A scrape can raise the following exceptions:</p>"},{"location":"api/#toomanytokens","title":"<code>TooManyTokens</code>","text":"<p>Raised when the number of tokens being sent exceeds the maximum allowed.</p> <p>This indicates that the HTML is too large to be processed by the API.</p> <p>Tip</p> <p>Consider using the <code>css</code> or <code>xpath</code> selectors to reduce the number of tokens being sent, or use the <code>split_length</code> parameter to split the request into multiple requests if necessary.</p>"},{"location":"api/#badstop","title":"<code>BadStop</code>","text":"<p>Indicates that OpenAI ran out of space before the stop token was reached.</p> <p>Tip</p> <p>OpenAI considers both the input and the response tokens when determining if the token limit has been exceeded.</p> <p>If you are using <code>split_length</code>, consider decreasing the value to leave more space for responses.</p>"},{"location":"api/#invalidjson","title":"<code>InvalidJSON</code>","text":"<p>Indicates that the JSON returned by the API is invalid.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#030-wip","title":"0.3.0 - WIP","text":"<ul> <li>use <code>tiktoken</code> for tokenization instead of guessing</li> <li>compute cost of each call, add <code>total_cost</code> to scrapers</li> <li>add tests and complete examples</li> <li>list mode prompt improvments</li> </ul>"},{"location":"changelog/#020-2021-03-18","title":"0.2.0 - 2021-03-18","text":"<ul> <li>Add list mode, auto-splitting, and pagination support.</li> <li>Improve <code>xpath</code> and <code>css</code> handling.</li> <li>Improve prompt for GPT 3.5.</li> <li>Make it possible to alter parameters when calling scrape.</li> <li>Logging &amp; error handling.</li> <li>Command line interface.</li> <li>See blog post for details: https://jamesturk.net/posts/scraping-with-gpt-part-2/</li> </ul>"},{"location":"changelog/#010-2021-03-17","title":"0.1.0 - 2021-03-17","text":"<ul> <li>Initial experiment, see blog post for more: https://jamesturk.net/posts/scraping-with-gpt-4/</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>scrapeghost offers a command line interface which is particularly useful for experimentation.</p> <p>It is also possible to use as a step in a data pipeline.</p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>In order to use the CLI, the <code>OPENAI_API_KEY</code> environment variable must be set.</p>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>scrapeghost --help\n Usage: scrapeghost [OPTIONS] URL                                                                                                                                            \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    url      TEXT  [default: None] [required]                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --xpath                         TEXT     XPath selector to narrow the scrape [default: None]        \u2502\n\u2502 --css                           TEXT     CSS selector to narrow the scrape [default: None]          \u2502\n\u2502 --schema                        TEXT     Schema to use for scraping [default: None]                 \u2502\n\u2502 --schema-file                   PATH     Path to schema.json file [default: None]                   \u2502\n\u2502 --gpt4             --no-gpt4             Use GPT-4 instead of GPT-3.5-turbo [default: no-gpt4]      \u2502\n\u2502 --verbose      -v               INTEGER  Verbosity level 0-2 [default: 0]                           \u2502\n\u2502 --help                                   Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement:</p> <ul> <li>James Turk: dev@jamesturk.net</li> </ul> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-this-practical","title":"Is this practical?","text":"<p>When I started this project, I really didn't think it would be. I was aiming at a fun proof of concept, but I've been surprised by the results.</p> <p>Time will tell if this is a practical tool, but I'm somewhat hopeful now.</p>"},{"location":"faq/#why-not-use-a-different-model","title":"Why not use a different model?","text":"<p>This was a toy project, not an attempt to build a production system.  I'm open to trying other models if you have suggestions.</p>"},{"location":"faq/#what-about-pages-where-the-data-is-loaded-dynamically","title":"What about pages where the data is loaded dynamically?","text":"<p>This won't work for those out of the box.  It should be possible to use something like selenium to load the page and then pass the rendered HTML to <code>scrapeghost</code>.</p>"},{"location":"faq/#what-if-a-page-is-too-big","title":"What if a page is too big?","text":"<p>Try the following:</p> <ol> <li> <p>Provide a CSS or XPath selector to limit the scope of the page.</p> </li> <li> <p>Pre-process the HTML. Trim tags or entire sections you don't need.</p> </li> <li> <p>Finally, you can use the <code>split_length</code> parameter to split the page into smaller chunks.  This only works for list-type pages, and requires a good choice of selector to split the page up.</p> </li> </ol>"},{"location":"faq/#why-not-ask-the-scraper-to-write-css-xpath-selectors","title":"Why not ask the scraper to write CSS / XPath selectors?","text":"<p>While it'd seem like this would perform better, there are a few practical challenges standing in the way right now.</p> <ul> <li>Writing a robust CSS/XPath selector that'd run against a whole set of pages would require passing a lot of context to the model. The token limit is already the major limitation.</li> <li>The current solution does not require any changes when a page changes.  A selector-based model would require retraining every time a page changes as well as a means to detect such changes.</li> <li>For some data, selectors alone are not enough. The current model can easily extract all of the addresses from a page and break them into city/state/etc. A selector-based model would not be able to do this.</li> </ul> <p>I do think there is room for hybrid approaches, and I plan to continue to explore them.</p>"},{"location":"faq/#does-the-model-hallucinate-data","title":"Does the model \"hallucinate\" data?","text":"<p>It is possible, but in practice hasn't been observed as a major problem yet.</p> <p>Because the temperature is zero, the output is fully deterministic and seems less likely to hallucinate data.</p> <p>It is definitely possible however, and future versions of this tool will allow for automated error checking (and possibly correction).</p>"},{"location":"faq/#how-much-did-you-spend-developing-this","title":"How much did you spend developing this?","text":"<p>So far, about $25 on API calls, switching to GPT-3.5 as the default made a big difference.</p> <p>My most expensive call was a paginated GPT-4 call that cost $2.20.  I decided to add the cost-limiting features after that.</p>"},{"location":"faq/#whats-with-the-license","title":"What's with the license?","text":"<p>I'm still working on figuring this out.</p> <p>For now, if you're working in a commercial setting and the license scares you away, that's fine.</p> <p>If you really want to, you can contact me and we can work something out.</p>"},{"location":"openai/","title":"About the OpenAI API","text":"<p>This section assumes you are mostly unfamiliar with the OpenAI API and aims to provide a high-level overview of how they work in relation to this library. </p>"},{"location":"openai/#api-keys","title":"API Keys","text":"<p>To use the OpenAI APIs you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre>"},{"location":"openai/#costs","title":"Costs","text":"<p>The OpenAI APIs are considerably expensive. The cost of a call varies based on the model used and the size of the input.</p> <p>The cost estimates provided by this library are based on the OpenAI pricing page and not guaranteed to be accurate.</p> <p>Fortunately, https://platform.openai.com/account/billing/limits lets you set a soft &amp; hard limit so that you can't accidentally run up a large bill.</p> <p>It is highly recommended that you set a low usage limit on your API key to avoid accidentally running up a large bill.</p>"},{"location":"openai/#tokens","title":"Tokens","text":"<p>OpenAI encodes text using a tokenizer, which converts words to integers.</p> <p>You'll see that billing is based on the number of tokens used.  A token is approximately 3 characters, so 3000 characters of HTML will roughly correspond to 1000 tokens.</p> <p>Additionally, the GPT-3-Turbo model is limited to 4096 tokens.  GPT-4 is limited to 8192 tokens.  (A 32k model has been announced, but is not yet widely available.)</p> <p>Various features in the library will help you avoid running into token limits, but it is still very common to exceed them in practice.</p> <p>If your pages exceed them, you'll need to focus on improving your selectors so that only the required data is sent to the underlying models.</p>"},{"location":"openai/#prompts","title":"Prompts","text":"<p>The OpenAI APIs provide a chat-like interface, where there are three roles: system, user, and assistant.  The system commands provide guidance to the assistant on how it should perform its tasks.  The user provides a query to the assistant, which is then answered.</p> <p>In practice, this results in a prompt that looks like something this:</p> <p>System: For the given HTML, convert to a list of JSON objects matching this schema: <code>{\"name\": \"string\", \"age\": \"number\"}</code></p> <p>System: Be sure to provide valid JSON that is not truncated and contains no extra fields beyond those in the schema.</p> <p>User: <code>&lt;html&gt;&lt;div&gt;&lt;h2&gt;Joe&lt;/h2&gt;&lt;span&gt;Age: 42&lt;/span&gt;&lt;/div&gt;&lt;/html&gt;</code></p> <p>Assistant: <code>{\"name\": \"Joe\", \"age\": 42}</code></p> <p>It is possible to adjust the system commands the library sends, but the goal is to provide a simple default prompt that works well for most use cases.</p> <p>TODO: link to docs</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial will show you how to use <code>scrapeghost</code> to build a web scraper without writing page-specific code.</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<p>1) You'll need an OpenAI API key. You can get one here.</p> <p>2) You'll need to install <code>scrapeghost</code>. You can do this with <code>pip</code>, <code>poetry</code>, or your favorite Python package manager.</p>"},{"location":"tutorial/#set-the-api-key","title":"Set The API Key","text":"<p>It's generally easiest to set the API key as an environment variable.</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also put the key in a file, and have OpenAI read it from there.</p> <pre><code>import openai\nopenai.api_key_file = \"path/to/file\"\n</code></pre>"},{"location":"tutorial/#writing-a-scraper","title":"Writing a Scraper","text":"<p>The goal of our scraper is going to be to get a list of all of the episodes of the podcast Comedy Bang Bang.</p> <p>To do this, we'll need two kinds of scrapers: one to get a list of all of the episodes, and one to get the details of each episode.</p> <p>It's easiest to start with the scraper that gets the details of each episode.</p>"},{"location":"tutorial/#getting-episode-details","title":"Getting Episode Details","text":"<p>At the time of writing, the most recent episode of Comedy Bang Bang is Episode 800, Operation Golden Orb.</p> <p>The URL for this episode is https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb.</p> <p>If we view this page we can see there's a list of guests, a title, an episode number, release date, and a description.</p> <p>Let's say we want to build a scraper that finds out each episode's title, episode number, and release date.</p> <p>We can do this by creating a <code>SchemaScraper</code> object and passing it a schema.</p> <pre><code>from scrapeghost import SchemaScraper\n\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"str\",\n}\n\nepisode_scraper = SchemaScraper(schema)\n</code></pre> <p>There is no predefined way to define a schema, but a dictionary resembling the data you want to scrape where the keys are the names of the fields you want to scrape and the values are the types of the fields is a good place to start.</p> <p>Once you have an instance of <code>SchemaScraper</code> you can use it to scrape a specific page.</p> <pre><code>&gt;&gt;&gt; episode_scraper(\"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\")\n\nTooManyTokens: HTML is 10851 tokens, max for gpt-3.5-turbo is 4096\n</code></pre> <p>This error means that the content length is too long. A token is about 3 characters, give or take.</p>"},{"location":"tutorial/#dealing-with-token-limits","title":"Dealing with Token Limits","text":"<p>If you haven't used OpenAI's APIs before, you may not be aware of the token limits.  Every request has a limit on the number of tokens it can use. For GPT-4 this is 8,192 tokens. For GPT-3.5-Turbo it is 4,096.</p> <p>You are also billed per token, so even if you're under the limit, fewer tokens means cheaper API calls.</p> <p>So for example, a 4,000 token page that returns 1,000 tokens of JSON will cost $0.01 with GPT-3-Turbo, but $0.18 with GPT-4.</p> <p>Ideally, we'd only pass the relevant parts of the page to OpenAI. It shouldn't need anything outside of the HTML <code>&lt;body&gt;</code>, anything in comments, script tags, etc.</p> <p>The first thing <code>scrapeghost</code> does is clean irrelevant tags out of the page. In future versions, this will be configurable, but for now it uses <code>lxml.html.clean</code> which was removing about 30% of the tokens on a sample of pages seen during testing.</p> <p>As we saw above though, it's not uncommon for a page to still be over the limit.</p> <p>If you visit the page https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb viewing the source will reveal that all of the interesting content is in an element <code>&lt;div id=\"content\" class=\"page-content\"&gt;</code>.</p> <p>Just as we might if we were writing a real scraper, we'll write a CSS selector to grab this element, <code>div.page-content</code> will do.</p> <p>Now, we'll modify our call to our scraper to pass the CSS selector.</p> <pre><code>episode_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\",\n    css=\"div.page-content\",\n)\n</code></pre> <p>We can see from the logging output that the content length is much shorter now and we get a response:</p> <pre><code>{\n\"title\": \"Operation Golden Orb\",\n\"episode_number\": 800,\n\"release_date\": \"March 12, 2023\",\n}\n</code></pre> <p>All for less than a penny!</p> <p>Even when the page fits under the token limit, it's still a good idea to pass a selector to limit the amount of content that OpenAI has to process.</p>"},{"location":"tutorial/#enhancing-the-schema","title":"Enhancing the Schema","text":"<p>That was easy! Let's enhance our schema to include the list of guests as well as requesting the dates in a particular format.</p> <pre><code>schema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"YYYY-MM-DD\",\n    \"guests\": [{\"name\": \"str\"}],\n}\n\nepisode_scraper = SchemaScraper(schema)\nepisode_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\",\n    css=\"div.page-content\",\n)\n</code></pre> <pre><code>{\"title\": \"Operation Golden Orb\",\n\"episode_number\": 800,\n\"release_date\": \"2023-03-12\",\n\"guests\": [{\"name\": \"Jason Mantzoukas\"},\n{\"name\": \"Andy Daly\"},\n{\"name\": \"Paul F. Tompkins\"}]\n}\n</code></pre> <p>Let's try this on a different episode, from the beginning of the series.</p> <pre><code>episode_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Welcome_to_Comedy_Bang_Bang\",\n    css=\"div.page-content\",\n)\n</code></pre> <p>And there we have it!</p>"},{"location":"tutorial/#getting-a-list-of-episodes","title":"Getting a List of Episodes","text":"<p>Now that we have a scraper that can get the details of each episode, we need a scraper that can get a list of all of the episode URLs.</p> <p>https://comedybangbang.fandom.com/wiki/Category:Episodes has a link to each of the episodes, perhaps we can just scrape that page?</p> <pre><code>&gt;&gt;&gt; episode_list_scraper = SchemaScraper({\"episode_urls\": [\"str\"]})\n&gt;&gt;&gt; episode_list_scraper(\"https://comedybangbang.fandom.com/wiki/Category:Episodes\")\nTooManyTokens: HTML is 296830 tokens\n</code></pre> <p>Yikes, 296903 tokens! This is a huge page.</p> <p>We can try again with a CSS selector, but this time we'll try to get a selector for each individual item.</p> <p>If you have go this far, you may want to just extract links using <code>lxml.html</code> or <code>BeautifulSoup</code> instead.</p> <p>Let's imagine that for some reason you don't want to, perhaps this is a one-off project and even a relatively expensive request is worth it.</p> <p><code>SchemaScraper</code> has a few options that will help, we'll change our scraper to use <code>list_mode</code> and <code>split_length</code>.</p> <pre><code>episode_list_scraper = SchemaScraper(\n    \"url\",\n    list_mode=True,\n    split_length=2048,\n)\n</code></pre> <p><code>list_mode=True</code> alters the prompt and response format so that instead of returning a single JSON object, it returns a list of objects where each should match your provided <code>schema</code>.</p> <p>We alter the <code>schema</code> to just be a single string because we're only interested in the URL.</p> <p>Finally, we set the <code>split_length</code> to 5000. This is the maximum number of tokens that will be passed to OpenAI in a single request.</p> <p>TODO: recommendations</p> <pre><code>episode_urls = episode_list_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Category:Episodes\",\n    css=\".mw-parser-output a[class!='image link-internal']\",\n)\n</code></pre> <p>This winds up needing to make 25 requests, but gets the list of episode URLs.</p> <pre><code>print(episode_urls[:10])\nprint(episode_urls[-10:])\nprint(\"total:\", len(episode_urls))\nprint(\"cost:\", episode_list_scraper.total_cost)\n</code></pre> <p>It takes a while, but if you can stick to GPT-3.5-Turbo it's only $0.13.</p> <p>It isn't perfect, it is quite slow and also adding extra fields to the JSON, but it gets the job done.  It is trivial to combine the output of <code>episode_list_scraper</code> with <code>episode_scraper</code> to get the metadata for all of the episodes.</p> <p>See the listing at the very end for the full program.</p>"},{"location":"tutorial/#next-steps","title":"Next Steps","text":"<p>This is still very much a proof of concept, but it does work.</p> <p>I'm interested in ease of use and developer experience, as well as improving the accuracy.  If you want to follow along, the issues page is a good picture of what I'm considering next.</p>"},{"location":"tutorial/#putting-it-all-together","title":"Putting it all Together","text":"<pre><code>from scrapeghost import SchemaScraper\n\nepisode_list_scraper = SchemaScraper(\n    '{\"url\": \"url\"}',\n    list_mode=True,\n    split_length=2048,\n    # restrict this to GPT-3.5-Turbo to keep the cost down\n    models=[\"gpt-3.5-turbo\"],\n)\n\nepisode_scraper = SchemaScraper(\n    {\n        \"title\": \"str\",\n        \"episode_number\": \"int\",\n        \"release_date\": \"YYYY-MM-DD\",\n        \"guests\": [\"str\"],\n        \"characters\": [\"str\"],\n    },\n)\n\nepisode_urls = episode_list_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Category:Episodes\",\n    css=\".mw-parser-output a[class!='image link-internal']\",\n)\nprint(\n    f\"Scraped {len(episode_urls)} episode URLs, cost {episode_list_scraper.total_cost}\"\n)\n\nepisode_data = []\nfor episode_url in episode_urls:\n    print(episode_url)\n    episode_data.append(\n        episode_scraper(\n            episode_url[\"url\"],\n            css=\"div.page-content\",\n        )\n    )\n\nprint(f\"Scraped {len(episode_data)} episodes, cost {episode_scraper.total_cost}\")\n\nwith open(\"episode_data.json\", \"w\") as f:\n    json.dump(episode_data, f, indent=2)\n</code></pre>"}]}