{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p><code>scrapeghost</code> is an experimental library for scraping websites using OpenAI's GPT API.</p> <p>The library provides a means to scrape structured data from HTML without writing page-specific code.</p> <p>Important</p> <p>This library is very experimental with a rapidly evolving interface. No guarantees are made about the stability of the API or the accuracy of the results.</p> <p>Additionally, be aware of the potential costs before using this library.</p> <p>Use at your own risk.</p> <p>Currently licensed under Hippocratic License 3.0.   (See FAQ.)</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Step 1) Obtain an OpenAI API key (https://platform.openai.com) and set an environment variable:</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>Step 2) Install the library however you like:</p> <p><pre><code>pip install scrapeghost\n</code></pre> or <pre><code>poetry add scrapeghost\n</code></pre></p> <p>Step 3) Instantiate a <code>SchemaScraper</code> by defining the shape of the data you wish to extract:</p> <pre><code>from scrapeghost import SchemaScraper\nscrape_legislators = SchemaScraper(\n  schema={\n      \"name\": \"string\",\n      \"url\": \"url\",\n      \"district\": \"string\",\n      \"party\": \"string\",\n      \"photo_url\": \"url\",\n      \"offices\": [{\"name\": \"string\", \"address\": \"string\", \"phone\": \"string\"}],\n  }\n)\n</code></pre> <p>Note</p> <p>There's no pre-defined format for the schema, the GPT models do a good job of figuring out what you want and you can use whatever values you want to provide hints.</p> <p>Step 4) Passing the scraper a URL (or HTML) to the resulting scraper will return a dictionary of the scraped data:</p> <p><pre><code> scrape_legislators(\"https://www.ilga.gov/house/rep.asp?MemberID=3071\")\n</code></pre> <pre><code>{\"name\": \"Emanuel 'Chris' Welch\",\n\"url\": \"https://www.ilga.gov/house/Rep.asp?MemberID=3071\",\n\"district\": \"7th\", \"party\": \"D\", \"photo_url\": \"https://www.ilga.gov/images/members/{5D419B94-66B4-4F3B-86F1-BFF37B3FA55C}.jpg\",\n\"offices\": [\n{\"name\": \"Springfield Office\",\n\"address\": \"300 Capitol Building, Springfield, IL 62706\",\n\"phone\": \"(217) 782-5350\"},\n{\"name\": \"District Office\",\n\"address\": \"10055 W. Roosevelt Rd., Suite E, Westchester, IL 60154\",\n\"phone\": \"(708) 450-1000\"}\n]}\n</code></pre></p> <p>That's it!</p> <p>Read the tutorial for a step-by-step guide to building a scraper.</p>"},{"location":"#command-line-usage-example","title":"Command Line Usage Example","text":"<p>If you've installed the package (e.g. with <code>pipx</code>), you can use the <code>scrapeghost</code> command line tool to experiment.</p> <p><pre><code>#!/bin/sh \nscrapeghost https://www.ncleg.gov/Members/Biography/S/436  \\\n--schema \"{'first_name': 'str', 'last_name': 'str',\n        'photo_url': 'url', 'offices': [] }'\" \\\n--css div.card | python -m json.tool\n</code></pre> <pre><code>{\n\"first_name\": \"Gale\",\n\"last_name\": \"Adcock\",\n\"photo_url\": \"https://www.ncleg.gov/Members/MemberImage/S/436/Low\",\n\"offices\": [\n{\n\"type\": \"mailing\",\n\"address\": \"16 West Jones Street, Rm. 1104, Raleigh, NC 27601\"\n},\n{\n\"type\": \"phone\",\n\"number\": \"(919) 715-3036\"\n},\n{\n\"type\": \"email\",\n\"address\": \"Gale.Adcock@ncleg.gov\"\n}\n]\n}\n</code></pre></p> <p>See the CLI docs for more details.</p>"},{"location":"#features","title":"Features","text":"<p>The bulk of the work is of course done by the GPT models. The purpose of this library is to provide a convenient interface for using GPT for the purpose of web scraping.</p> <p>Python-based schema definition - Define the shape of the data you want to extract as any Python object.</p> <ul> <li>Future versions will support optional validation that the response matches the schema.</li> </ul> <p>Token Reduction - Fewer tokens means lower costs, faster responses, and staying under the API's token limits.</p> <ul> <li>Automatic HTML cleaning - Remove unnecessary HTML tags and attributes to reduce the size of the HTML sent to the model.</li> <li>CSS and XPath selectors - Pre-filter the HTML to send to the model by writing a single CSS or XPath selector.</li> <li>Auto-splitting - Optionally split the HTML into multiple calls to the model, each of a specified length.</li> </ul> <p>Cost Controls - Scrapers keep running totals of how many tokens have been sent and received, so costs can be tracked.</p> <ul> <li>Future versions will allow setting a budget and stopping the scraper if the budget is exceeded.</li> </ul> <p>Model Options - Works with GPT-3.5-Turbo or GPT 4, and allows passing additional parameters to the model to customize behavior.</p> <ul> <li>Support for automatic fallbacks (e.g. use cost-saving GPT-3.5-Turbo by default, fall back to GPT-4 if needed.)</li> </ul> <p>Error Handling &amp; Logging - Detailed logging and error handling to help debug issues.</p>"},{"location":"LICENSE/","title":"Hippocratic License","text":"<p>HIPPOCRATIC LICENSE</p> <p>Version 3.0, October 2021</p> <p>https://firstdonoharm.dev/version/3/0/extr-ffd-law-mil-sv.md</p> <p>TERMS AND CONDITIONS</p> <p>TERMS AND CONDITIONS FOR USE, COPY, MODIFICATION, PREPARATION OF DERIVATIVE WORK, REPRODUCTION, AND DISTRIBUTION:</p> <p>1. DEFINITIONS:</p> <p>This section defines certain terms used throughout this license agreement.</p> <p>1.1. \u201cLicense\u201d means the terms and conditions, as stated herein, for use, copy, modification, preparation of derivative work, reproduction, and distribution of Software (as defined below).</p> <p>1.2. \u201cLicensor\u201d means the copyright and/or patent owner or entity authorized by the copyright and/or patent owner that is granting the License.</p> <p>1.3. \u201cLicensee\u201d means the individual or entity exercising permissions granted by this License, including the use, copy, modification, preparation of derivative work, reproduction, and distribution of Software (as defined below).</p> <p>1.4. \u201cSoftware\u201d means any copyrighted work, including but not limited to software code, authored by Licensor and made available under this License.</p> <p>1.5. \u201cSupply Chain\u201d means the sequence of processes involved in the production and/or distribution of a commodity, good, or service offered by the Licensee.</p> <p>1.6. \u201cSupply Chain Impacted Party\u201d or \u201cSupply Chain Impacted Parties\u201d means any person(s) directly impacted by any of Licensee\u2019s Supply Chain, including the practices of all persons or entities within the Supply Chain prior to a good or service reaching the Licensee.</p> <p>1.7. \u201cDuty of Care\u201d is defined by its use in tort law, delict law, and/or similar bodies of law closely related to tort and/or delict law, including without limitation, a requirement to act with the watchfulness, attention, caution, and prudence that a reasonable person in the same or similar circumstances would use towards any Supply Chain Impacted Party.</p> <p>1.8. \u201cWorker\u201d is defined to include any and all permanent, temporary, and agency workers, as well as piece-rate, salaried, hourly paid, legal young (minors), part-time, night, and migrant workers.</p> <p>2. INTELLECTUAL PROPERTY GRANTS:</p> <p>This section identifies intellectual property rights granted to a Licensee.</p> <p>2.1. Grant of Copyright License: Subject to the terms and conditions of this License, Licensor hereby grants to Licensee a worldwide, non-exclusive, no-charge, royalty-free copyright license to use, copy, modify, prepare derivative work, reproduce, or distribute the Software, Licensor authored modified software, or other work derived from the Software.</p> <p>2.2. Grant of Patent License: Subject to the terms and conditions of this License, Licensor hereby grants Licensee a worldwide, non-exclusive, no-charge, royalty-free patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer Software.</p> <p>3. ETHICAL STANDARDS:</p> <p>This section lists conditions the Licensee must comply with in order to have rights under this License.</p> <p>The rights granted to the Licensee by this License are expressly made subject to the Licensee\u2019s ongoing compliance with the following conditions:</p> <ul> <li>3.1. The Licensee SHALL NOT, whether directly or indirectly, through agents or assigns:  </li> <li>3.1.1. Infringe upon any person\u2019s right to life or security of person, engage in extrajudicial killings, or commit murder, without lawful cause (See Article 3, United Nations Universal Declaration of Human Rights; Article 6, International Covenant on Civil and Political Rights)  </li> <li>3.1.2. Hold any person in slavery, servitude, or forced labor (See Article 4, United Nations Universal Declaration of Human Rights; Article 8, International Covenant on Civil and Political Rights);  </li> <li>3.1.3. Contribute to the institution of slavery, slave trading, forced labor, or unlawful child labor (See Article 4, United Nations Universal Declaration of Human Rights; Article 8, International Covenant on Civil and Political Rights);  </li> <li>3.1.4. Torture or subject any person to cruel, inhumane, or degrading treatment or punishment (See Article 5, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Civil and Political Rights);  </li> <li>3.1.5. Discriminate on the basis of sex, gender, sexual orientation, race, ethnicity, nationality, religion, caste, age, medical disability or impairment, and/or any other like circumstances (See Article 7, United Nations Universal Declaration of Human Rights; Article 2, International Covenant on Economic, Social and Cultural Rights; Article 26, International Covenant on Civil and Political Rights);  </li> <li>3.1.6. Prevent any person from exercising his/her/their right to seek an effective remedy by a competent court or national tribunal (including domestic judicial systems, international courts, arbitration bodies, and other adjudicating bodies) for actions violating the fundamental rights granted to him/her/them by applicable constitutions, applicable laws, or by this License (See Article 8, United Nations Universal Declaration of Human Rights; Articles 9 and 14, International Covenant on Civil and Political Rights);  </li> <li>3.1.7. Subject any person to arbitrary arrest, detention, or exile (See Article 9, United Nations Universal Declaration of Human Rights; Article 9, International Covenant on Civil and Political Rights);  </li> <li>3.1.8. Subject any person to arbitrary interference with a person\u2019s privacy, family, home, or correspondence without the express written consent of the person (See Article 12, United Nations Universal Declaration of Human Rights; Article 17, International Covenant on Civil and Political Rights);  </li> <li>3.1.9. Arbitrarily deprive any person of his/her/their property (See Article 17, United Nations Universal Declaration of Human Rights);  </li> <li>3.1.10. Forcibly remove indigenous peoples from their lands or territories or take any action with the aim or effect of dispossessing indigenous peoples from their lands, territories, or resources, including without limitation the intellectual property or traditional knowledge of indigenous peoples, without the free, prior, and informed consent of indigenous peoples concerned (See Articles 8 and 10, United Nations Declaration on the Rights of Indigenous Peoples);  </li> <li>3.1.11. Fossil Fuel Divestment: Be an individual or entity, or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, on the FFI Solutions Carbon Underground 200 list;  </li> <li>3.1.12. Extractive Industries: Be an individual or entity, or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, that engages in fossil fuel or mineral exploration, extraction, development, or sale;  </li> <li>3.1.13. Mass Surveillance: Be a government agency or multinational corporation, or a representative, agent, affiliate, successor, attorney, or assign of a government or multinational corporation, which participates in mass surveillance programs;  </li> <li>3.1.14. Military Activities: Be an entity or a representative, agent, affiliate, successor, attorney, or assign of an entity which conducts military activities;  </li> <li>3.1.15. Law Enforcement: Be an individual or entity, or a or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, that provides good or services to, or otherwise enters into any commercial contracts with, any local, state, or federal law enforcement agency;  </li> <li>3.1.16. Interfere with Workers' free exercise of the right to organize and associate (See Article 20, United Nations Universal Declaration of Human Rights; C087 - Freedom of Association and Protection of the Right to Organise Convention, 1948 (No. 87), International Labour Organization; Article 8, International Covenant on Economic, Social and Cultural Rights); and  </li> <li>3.1.17. Harm the environment in a manner inconsistent with local, state, national, or international law.</li> <li>3.2. The Licensee SHALL:  </li> <li>3.2.1. Provide equal pay for equal work where the performance of such work requires equal skill, effort, and responsibility, and which are performed under similar working conditions, except where such payment is made pursuant to:  <ul> <li>3.2.1.1. A seniority system;</li> <li>3.2.1.2. A merit system;  </li> <li>3.2.1.3. A system which measures earnings by quantity or quality of production; or</li> <li>3.2.1.4. A differential based on any other factor other than sex, gender, sexual orientation, race, ethnicity, nationality, religion, caste, age, medical disability or impairment, and/or any other like circumstances (See 29 U.S.C.A. \u00a7 206(d)(1); Article 23, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Economic, Social and Cultural Rights; Article 26, International Covenant on Civil and Political Rights); and  </li> </ul> </li> <li>3.2.2. Allow for reasonable limitation of working hours and periodic holidays with pay (See Article 24, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Economic, Social and Cultural Rights).</li> </ul> <p>4. SUPPLY CHAIN IMPACTED PARTIES:</p> <p>This section identifies additional individuals or entities that a Licensee could harm as a result of violating the Ethical Standards section, the condition that the Licensee must voluntarily accept a Duty of Care for those individuals or entities, and the right to a private right of action that those individuals or entities possess as a result of violations of the Ethical Standards section.</p> <p>4.1. In addition to the above Ethical Standards, Licensee voluntarily accepts a Duty of Care for Supply Chain Impacted Parties of this License, including individuals and communities impacted by violations of the Ethical Standards. The Duty of Care is breached when a provision within the Ethical Standards section is violated by a Licensee, one of its successors or assigns, or by an individual or entity that exists within the Supply Chain prior to a good or service reaching the Licensee.</p> <p>4.2. Breaches of the Duty of Care, as stated within this section, shall create a private right of action, allowing any Supply Chain Impacted Party harmed by the Licensee to take legal action against the Licensee in accordance with applicable negligence laws, whether they be in tort law, delict law, and/or similar bodies of law closely related to tort and/or delict law, regardless if Licensee is directly responsible for the harms suffered by a Supply Chain Impacted Party. Nothing in this section shall be interpreted to include acts committed by individuals outside of the scope of his/her/their employment.</p> <p>5. NOTICE: This section explains when a Licensee must notify others of the License.</p> <p>5.1. Distribution of Notice: Licensee must ensure that everyone who receives a copy of or uses any part of Software from Licensee, with or without changes, also receives the License and the copyright notice included with Software (and if included by the Licensor, patent, trademark, and attribution notice). Licensee must ensure that License is prominently displayed so that any individual or entity seeking to download, copy, use, or otherwise receive any part of Software from Licensee is notified of this License and its terms and conditions. Licensee must cause any modified versions of the Software to carry prominent notices stating that Licensee changed the Software.</p> <p>5.2. Modified Software: Licensee is free to create modifications of the Software and distribute only the modified portion created by Licensee, however, any derivative work stemming from the Software or its code must be distributed pursuant to this License, including this Notice provision.</p> <p>5.3. Recipients as Licensees: Any individual or entity that uses, copies, modifies, reproduces, distributes, or prepares derivative work based upon the Software, all or part of the Software\u2019s code, or a derivative work developed by using the Software, including a portion of its code, is a Licensee as defined above and is subject to the terms and conditions of this License.</p> <p>6. REPRESENTATIONS AND WARRANTIES:</p> <p>6.1. Disclaimer of Warranty: TO THE FULL EXTENT ALLOWED BY LAW, THIS SOFTWARE COMES \u201cAS IS,\u201d WITHOUT ANY WARRANTY, EXPRESS OR IMPLIED, AND LICENSOR SHALL NOT BE LIABLE TO ANY PERSON OR ENTITY FOR ANY DAMAGES OR OTHER LIABILITY ARISING FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THIS LICENSE, UNDER ANY LEGAL CLAIM.</p> <p>6.2. Limitation of Liability: LICENSEE SHALL HOLD LICENSOR HARMLESS AGAINST ANY AND ALL CLAIMS, DEBTS, DUES, LIABILITIES, LIENS, CAUSES OF ACTION, DEMANDS, OBLIGATIONS, DISPUTES, DAMAGES, LOSSES, EXPENSES, ATTORNEYS' FEES, COSTS, LIABILITIES, AND ALL OTHER CLAIMS OF EVERY KIND AND NATURE WHATSOEVER, WHETHER KNOWN OR UNKNOWN, ANTICIPATED OR UNANTICIPATED, FORESEEN OR UNFORESEEN, ACCRUED OR UNACCRUED, DISCLOSED OR UNDISCLOSED, ARISING OUT OF OR RELATING TO LICENSEE\u2019S USE OF THE SOFTWARE. NOTHING IN THIS SECTION SHOULD BE INTERPRETED TO REQUIRE LICENSEE TO INDEMNIFY LICENSOR, NOR REQUIRE LICENSOR TO INDEMNIFY LICENSEE.</p> <p>7. TERMINATION</p> <p>7.1. Violations of Ethical Standards or Breaching Duty of Care: If Licensee violates the Ethical Standards section or Licensee, or any other person or entity within the Supply Chain prior to a good or service reaching the Licensee, breaches its Duty of Care to Supply Chain Impacted Parties, Licensee must remedy the violation or harm caused by Licensee within 30 days of being notified of the violation or harm. If Licensee fails to remedy the violation or harm within 30 days, all rights in the Software granted to Licensee by License will be null and void as between Licensor and Licensee.</p> <p>7.2. Failure of Notice: If any person or entity notifies Licensee in writing that Licensee has not complied with the Notice section of this License, Licensee can keep this License by taking all practical steps to comply within 30 days after the notice of noncompliance. If Licensee does not do so, Licensee\u2019s License (and all rights licensed hereunder) will end immediately.</p> <p>7.3. Judicial Findings: In the event Licensee is found by a civil, criminal, administrative, or other court of competent jurisdiction, or some other adjudicating body with legal authority, to have committed actions which are in violation of the Ethical Standards or Supply Chain Impacted Party sections of this License, all rights granted to Licensee by this License will terminate immediately.</p> <p>7.4. Patent Litigation: If Licensee institutes patent litigation against any entity (including a cross-claim or counterclaim in a suit) alleging that the Software, all or part of the Software\u2019s code, or a derivative work developed using the Software, including a portion of its code, constitutes direct or contributory patent infringement, then any patent license, along with all other rights, granted to Licensee under this License will terminate as of the date such litigation is filed.</p> <p>7.5. Additional Remedies: Termination of the License by failing to remedy harms in no way prevents Licensor or Supply Chain Impacted Party from seeking appropriate remedies at law or in equity.</p> <p>8. MISCELLANEOUS:</p> <p>8.1. Conditions: Sections 3, 4.1, 5.1, 5.2, 7.1, 7.2, 7.3, and 7.4 are conditions of the rights granted to Licensee in the License.</p> <p>8.2. Equitable Relief: Licensor and any Supply Chain Impacted Party shall be entitled to equitable relief, including injunctive relief or specific performance of the terms hereof, in addition to any other remedy to which they are entitled at law or in equity.</p> <p>8.3. Severability: If any term or provision of this License is determined to be invalid, illegal, or unenforceable by a court of competent jurisdiction, any such determination of invalidity, illegality, or unenforceability shall not affect any other term or provision of this License or invalidate or render unenforceable such term or provision in any other jurisdiction. If the determination of invalidity, illegality, or unenforceability by a court of competent jurisdiction pertains to the terms or provisions contained in the Ethical Standards section of this License, all rights in the Software granted to Licensee shall be deemed null and void as between Licensor and Licensee.</p> <p>8.4. Section Titles: Section titles are solely written for organizational purposes and should not be used to interpret the language within each section.</p> <p>8.5. Citations: Citations are solely written to provide context for the source of the provisions in the Ethical Standards.</p> <p>8.6. Section Summaries: Some sections have a brief italicized description which is provided for the sole purpose of briefly describing the section and should not be used to interpret the terms of the License.</p> <p>8.7. Entire License: This is the entire License between the Licensor and Licensee with respect to the claims released herein and that the consideration stated herein is the only consideration or compensation to be paid or exchanged between them for this License. This License cannot be modified or amended except in a writing signed by Licensor and Licensee.</p> <p>8.8. Successors and Assigns: This License shall be binding upon and inure to the benefit of the Licensor\u2019s and Licensee\u2019s respective heirs, successors, and assigns.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#schemascraper","title":"<code>SchemaScraper</code>","text":"<p>The <code>SchemaScraper</code> class is the main interface to the API.</p> <p>It has one required parameter:</p> <ul> <li><code>schema</code> - A dictionary describing the shape of the data you wish to extract.</li> </ul> <p>And the following optional parameters:</p> <ul> <li><code>models</code> - list[str] - A list of models to use, in order of preference.  Defaults to <code>[\"gpt-3.5-turbo\", \"gpt-4\"]</code>.  </li> <li><code>model_params</code> - dict - A dictionary of parameters to pass to the underlying GPT model.  (See OpenAI docs for details.)</li> <li><code>max_cost</code> -  float (dollars) - The maximum total cost of calls made using this scraper. This is set to 1 ($1.00) by default to avoid large unexpected charges.</li> <li><code>list_mode</code> - bool - If <code>True</code>, the instructions and behavior will be slightly modified to better perform on pages with lists of similar items.</li> <li><code>extra_instructions</code> - list[str] - Additional instructions to pass to the GPT model as a system prompt.</li> <li><code>preprocessors</code> - list - A list of preprocessors to run on the HTML before sending it to the API. </li> <li><code>split_length</code> - int - If set, the scraper will split the page into multiple calls, each of this length. See auto-splitting for details.</li> </ul>"},{"location":"api/#preprocessors","title":"Preprocessors","text":"<p>Preprocessors allow you to modify the HTML before it is sent to the API.</p> <p>Three preprocessors are included by default:</p> <ul> <li><code>CleanHTML</code> - Cleans the HTML using <code>lxml.html.clean.Cleaner</code>.</li> <li><code>XPath</code> - Applies an XPath selector to the HTML.</li> <li><code>CSS</code> - Applies a CSS selector to the HTML.</li> </ul> <p>Note: <code>CleanHTML</code> is always applied as it is part of <code>SchemaScraper._default_preprocessors</code>.</p> <p>You can add your own preprocessors by passing a list of callables to the <code>preprocessors</code> parameter.</p> <pre><code>scraper = SchemaScraper(schema, preprocessors=[CSS(\"table\")])\n</code></pre> <p>It is also possible to pass preprocessors at scrape time via the <code>extra_preprocessors</code> parameter:</p> <pre><code>scraper = SchemaScraper(schema)\nscraper.scrape(\"https://example.com\", extra_preprocessors=[CSS(\"table\")])\n</code></pre> <p>Implementing your own preprocessor is simple, just create a callable that takes a <code>lxml.html.HtmlElement</code> and returns a list of <code>lxml.html.HtmlElement</code> objects.  Look at <code>preprocessors.py</code> for examples.</p>"},{"location":"api/#auto-splitting","title":"Auto-splitting","text":"<p>It's worth mentioning how <code>split_length</code> works because it allows for some interesting possibilities but can also become quite expensive.</p> <p>If you pass <code>split_length</code> to the scraper, it assumes the page is made of multiple similar sections and will try to split the page into multiple calls.  </p> <p>When you call the scrape function of an auto-splitting enabled scraper, it is important to use a splitting preprocessor like <code>XPath</code> or <code>CSS</code>.  The resulting nodes will be combined into chunks no bigger than <code>split_length</code> tokens, sent to the API, and then stitched back together.</p> <p>This seems to work well for long lists of similar items, though whether it is worth the many calls is questionable.</p>"},{"location":"api/#scrape","title":"<code>scrape</code>","text":"<p>The <code>scrape</code> method of a <code>SchemaScraper</code> is used to scrape a page.</p> <pre><code>scraper = SchemaScraper(schema)\nscraper.scrape(\"https://example.com\")\n</code></pre> <ul> <li><code>url_or_html</code> - The first parameter should be a URL or HTML string to scrape.</li> <li><code>extra_preprocessors</code> - A list of preprocessors to run on the HTML before sending it to the API.</li> </ul> <p>It is also possible to call the scraper directly, which is equivalent to calling <code>scrape</code>:</p> <pre><code>scraper = SchemaScraper(schema)\nscraper(\"https://example.com\")\n# same as writing\nscraper.scrape(\"https://example.com\")\n</code></pre>"},{"location":"api/#paginatedschemascraper","title":"<code>PaginatedSchemaScraper</code>","text":"<p>TODO: document this</p>"},{"location":"api/#exceptions","title":"Exceptions","text":"<p>The following exceptions can be raised by the scraper:</p> <p>(all are subclasses of <code>ScrapeghostError</code>)</p>"},{"location":"api/#maxcostexceeded","title":"<code>MaxCostExceeded</code>","text":"<p>The maximum cost of the scraper has been exceeded.</p> <p>Raise the <code>max_cost</code> parameter to allow more calls to be made.</p>"},{"location":"api/#preprocessorerror","title":"<code>PreprocessorError</code>","text":"<p>A preprocessor encountered an error (such as returning an empty list of nodes).</p>"},{"location":"api/#toomanytokens","title":"<code>TooManyTokens</code>","text":"<p>Raised when the number of tokens being sent exceeds the maximum allowed.</p> <p>This indicates that the HTML is too large to be processed by the API.</p> <p>Tip</p> <p>Consider using the <code>css</code> or <code>xpath</code> selectors to reduce the number of tokens being sent, or use the <code>split_length</code> parameter to split the request into multiple requests if necessary.</p>"},{"location":"api/#badstop","title":"<code>BadStop</code>","text":"<p>Indicates that OpenAI ran out of space before the stop token was reached.</p> <p>Tip</p> <p>OpenAI considers both the input and the response tokens when determining if the token limit has been exceeded.</p> <p>If you are using <code>split_length</code>, consider decreasing the value to leave more space for responses.</p>"},{"location":"api/#invalidjson","title":"<code>InvalidJSON</code>","text":"<p>Indicates that the JSON returned by the API is invalid.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#030-wip","title":"0.3.0 - WIP","text":"<ul> <li>Add tests, docs, and complete examples!</li> <li>Add preprocessors to <code>SchemaScraper</code> to allow for uniform interface for cleaning &amp; selecting HTML.</li> <li>Use <code>tiktoken</code> for accurate token counts.</li> <li>New <code>cost_estimate</code> utility function.</li> <li>Cost is now tracked on a per-scraper basis (see the <code>total_cost</code> attribute on <code>SchemaScraper</code> objects).</li> <li><code>SchemaScraper</code> now takes a <code>max_cost</code> parameter to limit the total cost of a scraper.</li> <li>Prompt improvements, list mode simplification.</li> </ul>"},{"location":"changelog/#020-2021-03-18","title":"0.2.0 - 2021-03-18","text":"<ul> <li>Add list mode, auto-splitting, and pagination support.</li> <li>Improve <code>xpath</code> and <code>css</code> handling.</li> <li>Improve prompt for GPT 3.5.</li> <li>Make it possible to alter parameters when calling scrape.</li> <li>Logging &amp; error handling.</li> <li>Command line interface.</li> <li>See blog post for details: https://jamesturk.net/posts/scraping-with-gpt-part-2/</li> </ul>"},{"location":"changelog/#010-2021-03-17","title":"0.1.0 - 2021-03-17","text":"<ul> <li>Initial experiment, see blog post for more: https://jamesturk.net/posts/scraping-with-gpt-4/</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>scrapeghost offers a command line interface which is particularly useful for experimentation.</p> <p>It is also possible to use as a step in a data pipeline.</p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>In order to use the CLI, the <code>OPENAI_API_KEY</code> environment variable must be set.</p>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>scrapeghost --help\n Usage: scrapeghost [OPTIONS] URL                                                                                                                                            \n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    url      TEXT  [default: None] [required]                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --xpath                         TEXT     XPath selector to narrow the scrape [default: None]        \u2502\n\u2502 --css                           TEXT     CSS selector to narrow the scrape [default: None]          \u2502\n\u2502 --schema                        TEXT     Schema to use for scraping [default: None]                 \u2502\n\u2502 --schema-file                   PATH     Path to schema.json file [default: None]                   \u2502\n\u2502 --gpt4             --no-gpt4             Use GPT-4 instead of GPT-3.5-turbo [default: no-gpt4]      \u2502\n\u2502 --verbose      -v               INTEGER  Verbosity level 0-2 [default: 0]                           \u2502\n\u2502 --help                                   Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement:</p> <ul> <li>James Turk: dev@jamesturk.net</li> </ul> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-this-practical","title":"Is this practical?","text":"<p>When I started this project, I really didn't think it would be. I was aiming at a fun proof of concept, but I've been surprised by the results.</p> <p>Time will tell if this is a practical tool, but I'm somewhat hopeful now.</p>"},{"location":"faq/#why-not-use-a-different-model","title":"Why not use a different model?","text":"<p>This was a toy project, not an attempt to build a production system.  I'm open to trying other models if you have suggestions.</p>"},{"location":"faq/#what-about-pages-where-the-data-is-loaded-dynamically","title":"What about pages where the data is loaded dynamically?","text":"<p>This won't work for those out of the box.  It should be possible to use something like selenium to load the page and then pass the rendered HTML to <code>scrapeghost</code>.</p>"},{"location":"faq/#what-if-a-page-is-too-big","title":"What if a page is too big?","text":"<p>Try the following:</p> <ol> <li> <p>Provide a CSS or XPath selector to limit the scope of the page.</p> </li> <li> <p>Pre-process the HTML. Trim tags or entire sections you don't need.</p> </li> <li> <p>Finally, you can use the <code>split_length</code> parameter to split the page into smaller chunks.  This only works for list-type pages, and requires a good choice of selector to split the page up.</p> </li> </ol>"},{"location":"faq/#why-not-ask-the-scraper-to-write-css-xpath-selectors","title":"Why not ask the scraper to write CSS / XPath selectors?","text":"<p>While it'd seem like this would perform better, there are a few practical challenges standing in the way right now.</p> <ul> <li>Writing a robust CSS/XPath selector that'd run against a whole set of pages would require passing a lot of context to the model. The token limit is already the major limitation.</li> <li>The current solution does not require any changes when a page changes.  A selector-based model would require retraining every time a page changes as well as a means to detect such changes.</li> <li>For some data, selectors alone are not enough. The current model can easily extract all of the addresses from a page and break them into city/state/etc. A selector-based model would not be able to do this.</li> </ul> <p>I do think there is room for hybrid approaches, and I plan to continue to explore them.</p>"},{"location":"faq/#does-the-model-hallucinate-data","title":"Does the model \"hallucinate\" data?","text":"<p>It is possible, but in practice hasn't been observed as a major problem yet.</p> <p>Because the temperature is zero, the output is fully deterministic and seems less likely to hallucinate data.</p> <p>It is definitely possible however, and future versions of this tool will allow for automated error checking (and possibly correction).</p>"},{"location":"faq/#how-much-did-you-spend-developing-this","title":"How much did you spend developing this?","text":"<p>So far, about $25 on API calls, switching to GPT-3.5 as the default made a big difference.</p> <p>My most expensive call was a paginated GPT-4 call that cost $2.20.  I decided to add the cost-limiting features after that.</p>"},{"location":"faq/#whats-with-the-license","title":"What's with the license?","text":"<p>I'm still working on figuring this out.</p> <p>For now, if you're working in a commercial setting and the license scares you away, that's fine.</p> <p>If you really want to, you can contact me and we can work something out.</p>"},{"location":"openai/","title":"About the OpenAI API","text":"<p>This section assumes you are mostly unfamiliar with the OpenAI API and aims to provide a high-level overview of how they work in relation to this library. </p>"},{"location":"openai/#api-keys","title":"API Keys","text":""},{"location":"openai/#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the OpenAI API you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>It's strongly recommended that you set a usage limit on your API key to avoid accidentally running up a large bill.</p> <p>https://platform.openai.com/account/billing/limits lets you set usage limits to avoid unpleasant surprises.</p>"},{"location":"openai/#using-your-key","title":"Using your key","text":"<p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>$ export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Be careful not to expose this key to the public by checking it into a public repository.</p>"},{"location":"openai/#costs","title":"Costs","text":"<p>The OpenAI API is considerably expensive.</p> <p>The cost of a call varies based on the model used and the size of the input.</p> <p>The cost estimates provided by this library are based on the OpenAI pricing page and not guaranteed to be accurate.</p> <p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12 <p>(See OpenAI pricing page for latest info.)</p>"},{"location":"openai/#tokens","title":"Tokens","text":"<p>OpenAI encodes text using a tokenizer, which converts words to integers.</p> <p>You'll see that billing is based on the number of tokens used.  A token is approximately 3 characters, so 3000 characters of HTML will roughly correspond to 1000 tokens.</p> <p>Additionally, the GPT-3-Turbo model is limited to 4096 tokens.  GPT-4 is limited to 8192 tokens.  (A 32k model has been announced, but is not yet widely available.)</p> <p>Various features in the library will help you avoid running into token limits, but it is still very common to exceed them in practice.</p> <p>If your pages exceed them, you'll need to focus on improving your selectors so that only the required data is sent to the underlying models.</p>"},{"location":"openai/#prompts","title":"Prompts","text":"<p>The OpenAI API provides a chat-like interface, where there are three roles: system, user, and assistant.  The system commands provide guidance to the assistant on how it should perform its tasks.  The user provides a query to the assistant, which is then answered.</p> <p>In practice, this results in a prompt that looks like something this:</p> <p>System: For the given HTML, convert to a list of JSON objects matching this schema: <code>{\"name\": \"string\", \"age\": \"number\"}</code></p> <p>System: Be sure to provide valid JSON that is not truncated and contains no extra fields beyond those in the schema.</p> <p>User: <code>&lt;html&gt;&lt;div&gt;&lt;h2&gt;Joe&lt;/h2&gt;&lt;span&gt;Age: 42&lt;/span&gt;&lt;/div&gt;&lt;/html&gt;</code></p> <p>Assistant: <code>{\"name\": \"Joe\", \"age\": 42}</code></p> <p>It is possible to adjust the system commands the library sends, but the goal is to provide a simple default prompt that works well for most use cases.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial will show you how to use <code>scrapeghost</code> to build a web scraper without writing page-specific code.</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorial/#install-scrapeghost","title":"Install <code>scrapeghost</code>","text":"<p>You'll need to install <code>scrapeghost</code>. You can do this with <code>pip</code>, <code>poetry</code>, or your favorite Python package manager.</p>"},{"location":"tutorial/#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the OpenAI API you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>It's strongly recommended that you set a usage limit on your API key to avoid accidentally running up a large bill.</p> <p>https://platform.openai.com/account/billing/limits lets you set usage limits to avoid unpleasant surprises.</p>"},{"location":"tutorial/#using-your-key","title":"Using your key","text":"<p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>$ export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Be careful not to expose this key to the public by checking it into a public repository.</p>"},{"location":"tutorial/#writing-a-scraper","title":"Writing a Scraper","text":"<p>The goal of our scraper is going to be to get a list of all of the episodes of the podcast Comedy Bang Bang.</p> <p>To do this, we'll need two kinds of scrapers: one to get a list of all of the episodes, and one to get the details of each episode.</p>"},{"location":"tutorial/#getting-episode-details","title":"Getting Episode Details","text":"<p>At the time of writing, the most recent episode of Comedy Bang Bang is Episode 800, Operation Golden Orb.</p> <p>The URL for this episode is https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb.</p> <p>Let's say we want to build a scraper that finds out each episode's title, episode number, and release date.</p> <p>We can do this by creating a <code>SchemaScraper</code> object and passing it a schema.</p> <pre><code>from scrapeghost import SchemaScraper\nfrom pprint import pprint  # pretty print results\n\nurl = \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"str\",\n}\n\nepisode_scraper = SchemaScraper(schema)\n\npprint(episode_scraper(url))\n</code></pre> <p>There is no predefined way to define a schema, but a dictionary resembling the data you want to scrape where the keys are the names of the fields you want to scrape and the values are the types of the fields is a good place to start.</p> <p>Once you have an instance of <code>SchemaScraper</code> you can use it to scrape a specific page by passing it a URL (or HTML if you prefer/need to fetch the data another way).</p> <p>Running our code gives an error though:</p> <pre><code>scrapeghost.scrapers.TooManyTokens: HTML is 9710 tokens, max for gpt-3.5-turbo is 4096\n</code></pre> <p>This means that the content length is too long.</p>"},{"location":"tutorial/#what-are-tokens","title":"What Are Tokens?","text":"<p>If you haven't used OpenAI's APIs before, you may not be aware of the token limits.  Every request has a limit on the number of tokens it can use. For GPT-4 this is 8,192 tokens. For GPT-3.5-Turbo it is 4,096.  (A token is about three characters.)</p> <p>You are also billed per token, so even if you're under the limit, fewer tokens means cheaper API calls.</p> <p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12 <p>(See OpenAI pricing page for latest info.)</p> <p>So for example, a 4,000 token page that returns 1,000 tokens of JSON will cost $0.01 with GPT-3-Turbo, but $0.18 with GPT-4.</p> <p>Ideally, we'd only pass the relevant parts of the page to OpenAI. It shouldn't need anything outside of the HTML <code>&lt;body&gt;</code>, anything in comments, script tags, etc.</p> <p>(For more details on how this library interacts with OpenAI's API, see the OpenAI API page.)</p>"},{"location":"tutorial/#preprocessors","title":"Preprocessors","text":"<p>To help with all this, <code>scrapeghost</code> provides a way to preprocess the HTML before it is sent to OpenAI. This is done by passing a list of preprocessor callables to the <code>SchemaScraper</code> constructor.</p> <p>Info</p> <p>A <code>CleanHTML</code> preprocessor is included by default. This removes HTML comments, script tags, and style tags.</p> <p>If you visit the page https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb viewing the source will reveal that all of the interesting content is in an element <code>&lt;div id=\"content\" class=\"page-content\"&gt;</code>.</p> <p>Just as we might if we were writing a real scraper, we'll write a CSS selector to grab this element, <code>div.page-content</code> will do.  The <code>CSS</code> preprocessor will use this selector to extract the content of the element.</p> <pre><code>from scrapeghost import SchemaScraper, CSS\nfrom pprint import pprint\n\nurl = \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"str\",\n}\n\nepisode_scraper = SchemaScraper(\n    schema,\n# can pass preprocessor to constructor or at scrape time\npreprocessors=[CSS(\"div.page-content\")],\n)\n\npprint(episode_scraper(url))\n</code></pre> <p>Now, a call to our scraper will only pass the content of the <code>&lt;div&gt;</code> to OpenAI. We get the following output:</p> <pre><code>2023-03-20 03:54:16 [debug    ] got HTML                       length=165605 url=https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\n2023-03-20 03:54:16 [debug    ] preprocessor                   from_nodes=1 name=CleanHTML nodes=1\n2023-03-20 03:54:16 [debug    ] preprocessor                   from_nodes=1 name=CSS(div.page-content) nodes=1\n2023-03-20 03:54:16 [info     ] API request                    html_tokens=1275 messages=['For the given HTML, convert to a JSON object matching this schema: {\"title\": \"str\", \"episode_number\": \"int\", \"release_date\": \"str\"}', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 03:54:19 [info     ] API response                   completion_tokens=34 cost=0.0027960000000000003 duration=2.5170469284057617 finish_reason=stop prompt_tokens=1364\n{'episode_number': 800,\n 'release_date': 'March 12, 2023',\n 'title': 'Operation Golden Orb'}\n</code></pre> <p>We can see from the logging output that the content length is much shorter now and we get the data we were hoping for.</p> <p>All for less than a penny!</p> <p>Tip</p> <p>Even when the page fits under the token limit, it is still a good idea to pass a selector to limit the amount of content that OpenAI has to process.</p>"},{"location":"tutorial/#enhancing-the-schema","title":"Enhancing the Schema","text":"<p>That was easy! Let's enhance our schema to include the list of guests as well as requesting the dates in a particular format.</p> <pre><code>from scrapeghost import SchemaScraper, CSS\nfrom pprint import pprint\n\nurl = \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n\"release_date\": \"YYYY-MM-DD\",\n\"guests\": [{\"name\": \"str\"}],\n}\n\nepisode_scraper = SchemaScraper(\n    schema,\n    preprocessors=[CSS(\"div.page-content\")],\n)\n\npprint(episode_scraper(url))\n</code></pre> <p>Just two small changes, but now we get the following output:</p> <pre><code>2023-03-20 04:02:07 [debug    ] got HTML                       length=165605 url=https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\n2023-03-20 04:02:07 [debug    ] preprocessor                   from_nodes=1 name=CleanHTML nodes=1\n2023-03-20 04:02:07 [debug    ] preprocessor                   from_nodes=1 name=CSS(div.page-content) nodes=1\n2023-03-20 04:02:07 [info     ] API request                    html_tokens=1275 messages=['For the given HTML, convert to a JSON object matching this schema: {\"title\": \"str\", \"episode_number\": \"int\", \"release_date\": \"YYYY-MM-DD\", \"guests\": [{\"name\": \"str\"}]}', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:02:12 [info     ] API response                   completion_tokens=84 cost=0.002922 duration=5.070535898208618 finish_reason=stop prompt_tokens=1377\n{'episode_number': 800,\n 'guests': [{'name': 'Jason Mantzoukas'},\n            {'name': 'Andy Daly'},\n            {'name': 'Paul F. Tompkins'}],\n 'release_date': '2023-03-12',\n 'title': 'Operation Golden Orb'}\n</code></pre> <p>Let's try this on a different episode, from the beginning of the series.</p> <p><pre><code>episode_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Welcome_to_Comedy_Bang_Bang\",\n)\n</code></pre> <pre><code>{'episode_number': 1,\n 'guests': [{'name': 'Rob Huebel'},\n            {'name': 'Tom Lennon'},\n            {'name': 'Doug Benson'}],\n 'release_date': '2009-05-01',\n 'title': 'Welcome to Comedy Bang Bang'}\n</code></pre></p> <p>And there we have it!</p>"},{"location":"tutorial/#getting-a-list-of-episodes","title":"Getting a List of Episodes","text":"<p>Now that we have a scraper that can get the details of each episode, we need a scraper that can get a list of all of the episode URLs.</p> <p>https://comedybangbang.fandom.com/wiki/Category:Episodes has a link to each of the episodes, perhaps we can just scrape that page?</p> <p><pre><code>from scrapeghost import SchemaScraper, CSS\n\nepisode_list_scraper = SchemaScraper({\"episode_urls\": [\"str\"]})\nepisode_list_scraper(\"https://comedybangbang.fandom.com/wiki/Category:Episodes\")\n</code></pre> <pre><code>scrapeghost.scrapers.TooManyTokens: HTML is 292918 tokens, max for gpt-3.5-turbo is 4096\n</code></pre></p> <p>Yikes, nearly 300k tokens! This is a huge page.</p> <p>We can try again with a CSS selector, but this time we'll try to get a selector for each individual item.</p> <p>If you have go this far, you may want to just extract links using <code>lxml.html</code> or <code>BeautifulSoup</code> instead.</p> <p>But let's imagine that for some reason you don't want to, perhaps this is a one-off project and even a relatively expensive request is worth it.</p> <p><code>SchemaScraper</code> has a few options that will help, we'll change our scraper to use <code>list_mode</code> and <code>split_length</code>.</p> <pre><code>from scrapeghost import SchemaScraper, CSS\n\nepisode_list_scraper = SchemaScraper(\n    \"url\",\n    list_mode=True,\n    split_length=2048,\n    preprocessors=[CSS(\".mw-parser-output a[class!='image link-internal']\")],\n)\nepisode_urls = episode_list_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Category:Episodes\"\n)\n\nprint(episode_urls[:3])\nprint(episode_urls[-3:])\nprint(\"total:\", len(episode_urls))\nprint(\"cost:\", episode_list_scraper.total_cost)\n</code></pre> <p><code>list_mode=True</code> alters the prompt and response format so that instead of returning a single JSON object, it returns a list of objects where each should match your provided <code>schema</code>.</p> <p>We alter the <code>schema</code> to just be a single string because we're only interested in the URL.</p> <p>Finally, we set the <code>split_length</code> to 2048. This is the maximum number of tokens that will be passed to OpenAI in a single request.</p> <p>It's a good idea to set this to about half the token limit, since the response counts against the token limit as well.</p> <p>This winds up needing to make over twenty requests, but gets the list of episode URLs after a few minutes.</p> <pre><code>2023-03-20 04:25:45 [debug    ] got HTML                       length=1425572 url=https://comedybangbang.fandom.com/wiki/Category:Episodes\n2023-03-20 04:25:45 [debug    ] preprocessor                   from_nodes=1 name=CleanHTML nodes=1\n2023-03-20 04:25:45 [debug    ] preprocessor                   from_nodes=1 name=CSS(.mw-parser-output a[class!='image link-internal']) nodes=857\n2023-03-20 04:25:45 [debug    ] chunked tags                   num=19 sizes=[2008, 2003, 2016, 2037, 2047, 2038, 2040, 2010, 2026, 2046, 2030, 2023, 1787, 2023, 2013, 2024, 2044, 2036, 2002]\n2023-03-20 04:25:45 [info     ] API request                    html_tokens=2008 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:29:45 [info     ] API response                   completion_tokens=1325 cost=0.006814000000000001 duration=239.47248005867004 finish_reason=stop prompt_tokens=2082\n2023-03-20 04:29:45 [info     ] API request                    html_tokens=2003 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:30:51 [info     ] API response                   completion_tokens=1278 cost=0.00671 duration=66.18540406227112 finish_reason=stop prompt_tokens=2077\n2023-03-20 04:30:51 [info     ] API request                    html_tokens=2016 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:31:47 [info     ] API response                   completion_tokens=971 cost=0.006122 duration=55.91373896598816 finish_reason=stop prompt_tokens=2090\n2023-03-20 04:31:47 [info     ] API request                    html_tokens=2037 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:33:49 [info     ] API response                   completion_tokens=1986 cost=0.008194 duration=122.70038199424744 finish_reason=length prompt_tokens=2111\n2023-03-20 04:33:49 [warning  ] API request failed             model=gpt-3.5-turbo\nOpenAI did not stop: length (prompt_tokens=2111, completion_tokens=1986)\n2023-03-20 04:33:49 [info     ] API request                    html_tokens=2037 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-4\n2023-03-20 04:37:32 [info     ] API response                   completion_tokens=1123 cost=0.16017 duration=222.54617595672607 finish_reason=stop prompt_tokens=2108\n2023-03-20 04:37:32 [info     ] API request                    html_tokens=2047 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:38:25 [info     ] API response                   completion_tokens=979 cost=0.006200000000000001 duration=53.06689095497131 finish_reason=stop prompt_tokens=2121\n2023-03-20 04:38:25 [info     ] API request                    html_tokens=2038 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:39:23 [info     ] API response                   completion_tokens=1003 cost=0.00623 duration=57.5164520740509 finish_reason=stop prompt_tokens=2112\n2023-03-20 04:39:23 [info     ] API request                    html_tokens=2040 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n2023-03-20 04:40:22 [info     ] API response                   completion_tokens=1012 cost=0.006252000000000001 duration=59.186081647872925 finish_reason=stop prompt_tokens=2114\n2023-03-20 04:40:22 [info     ] API request                    html_tokens=2010 messages=['For the given HTML, convert to a list of JSON objects matching this schema: \"url\"', 'Responses should be valid JSON, with no other text. Never truncate the JSON with an ellipsis. Always use double quotes for strings and escape quotes with \\\\. Always omit trailing commas. '] model=gpt-3.5-turbo\n</code></pre> <p>It takes a while, but if you can stick to GPT-3.5-Turbo it's only $0.13.</p> <p>It isn't perfect, it is quite slow and also adding extra fields to the JSON, but it gets the job done.  It is trivial to combine the output of <code>episode_list_scraper</code> with <code>episode_scraper</code> to get the metadata for all of the episodes.</p> <p>If you want to see the full code, jump down to the Putting it all Together section.</p>"},{"location":"tutorial/#next-steps","title":"Next Steps","text":"<p>If you're planning to use this library, please know it is very much in flux and I can't commit to API stability yet.</p> <p>If you are going to try to scrape using GPT, it'd probably be good to read the OpenAI API page to understand a little more about how the underlying API works.</p> <p>I'm still exploring ways to improve accuracy and the developer experience.  If you want to follow along, the issues page is a good picture of what I'm considering next.</p> <p>To see what other features are currently available, check out the API Reference.</p> <p>You can also explore the command line interface to see how you can use this library without writing any Python.</p>"},{"location":"tutorial/#putting-it-all-together","title":"Putting it all Together","text":"<pre><code>import json\nfrom scrapeghost import SchemaScraper, CSS\n\nepisode_list_scraper = SchemaScraper(\n    '{\"url\": \"url\"}',\n    list_mode=True,\n    split_length=2048,\n    # restrict this to GPT-3.5-Turbo to keep the cost down\n    models=[\"gpt-3.5-turbo\"],\n    preprocessors=CSS(\".mw-parser-output a[class!='image link-internal']\"),\n)\n\nepisode_scraper = SchemaScraper(\n    {\n        \"title\": \"str\",\n        \"episode_number\": \"int\",\n        \"release_date\": \"YYYY-MM-DD\",\n        \"guests\": [\"str\"],\n        \"characters\": [\"str\"],\n    },\n    preprocessors=CSS(\"div.page-content\"),\n)\n\nepisode_urls = episode_list_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Category:Episodes\",\n)\nprint(\n    f\"Scraped {len(episode_urls)} episode URLs, cost {episode_list_scraper.total_cost}\"\n)\n\nepisode_data = []\nfor episode_url in episode_urls:\n    print(episode_url)\n    episode_data.append(\n        episode_scraper(\n            episode_url[\"url\"],\n        )\n    )\n\nprint(f\"Scraped {len(episode_data)} episodes, cost {episode_scraper.total_cost}\")\n\nwith open(\"episode_data.json\", \"w\") as f:\n    json.dump(episode_data, f, indent=2)\n</code></pre>"},{"location":"snippets/_apikey/","title":"apikey","text":""},{"location":"snippets/_apikey/#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the OpenAI API you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>It's strongly recommended that you set a usage limit on your API key to avoid accidentally running up a large bill.</p> <p>https://platform.openai.com/account/billing/limits lets you set usage limits to avoid unpleasant surprises.</p>"},{"location":"snippets/_apikey/#using-your-key","title":"Using your key","text":"<p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>$ export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Be careful not to expose this key to the public by checking it into a public repository.</p>"},{"location":"snippets/_cost/","title":"cost","text":"<p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12 <p>(See OpenAI pricing page for latest info.)</p>"}]}