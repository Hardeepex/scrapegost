{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p><code>scrapeghost</code> is an experimental library for scraping websites using OpenAI's GPT.</p> <p>The library provides a means to scrape structured data from HTML without writing page-specific code.</p> <p>Important</p> <p>Before you proceed, here are at least three reasons why you should not use this library:</p> <ul> <li> <p>It is very experimental, no guarantees are made about the stability of the API or the accuracy of the results.</p> </li> <li> <p>It relies on the OpenAI API, which is quite slow and can be expensive.  (See costs before using this library.)</p> </li> <li> <p>Currently licensed under Hippocratic License 3.0.   (See FAQ.)</p> </li> </ul> <p>Use at your own risk.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Step 1) Obtain an OpenAI API key (https://platform.openai.com) and set an environment variable:</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>Step 2) Install the library however you like:</p> <p><pre><code>pip install scrapeghost\n</code></pre> or <pre><code>poetry add scrapeghost\n</code></pre></p> <p>Step 3) Instantiate a <code>SchemaScraper</code> by defining the shape of the data you wish to extract:</p> <pre><code>from scrapeghost import SchemaScraper\nscrape_legislators = SchemaScraper(\n  schema={\n      \"name\": \"string\",\n      \"url\": \"url\",\n      \"district\": \"string\",\n      \"party\": \"string\",\n      \"photo_url\": \"url\",\n      \"offices\": [{\"name\": \"string\", \"address\": \"string\", \"phone\": \"string\"}],\n  }\n)\n</code></pre> <p>Note</p> <p>There's no pre-defined format for the schema, the GPT models do a good job of figuring out what you want and you can use whatever values you want to provide hints.</p> <p>Step 4) Passing the scraper a URL (or HTML) to the resulting scraper will return the scraped data:</p> <p><pre><code>resp = scrape_legislators(\"https://www.ilga.gov/house/rep.asp?MemberID=3071\")\nresp.data\n</code></pre> <pre><code>{\"name\": \"Emanuel 'Chris' Welch\",\n\"url\": \"https://www.ilga.gov/house/Rep.asp?MemberID=3071\",\n\"district\": \"7th\", \"party\": \"D\", \"photo_url\": \"https://www.ilga.gov/images/members/{5D419B94-66B4-4F3B-86F1-BFF37B3FA55C}.jpg\",\n\"offices\": [\n{\"name\": \"Springfield Office\",\n\"address\": \"300 Capitol Building, Springfield, IL 62706\",\n\"phone\": \"(217) 782-5350\"},\n{\"name\": \"District Office\",\n\"address\": \"10055 W. Roosevelt Rd., Suite E, Westchester, IL 60154\",\n\"phone\": \"(708) 450-1000\"}\n]}\n</code></pre></p> <p>That's it!</p> <p>Read the tutorial for a step-by-step guide to building a scraper.</p>"},{"location":"#command-line-usage-example","title":"Command Line Usage Example","text":"<p>If you've installed the package (e.g. with <code>pipx</code>), you can use the <code>scrapeghost</code> command line tool to experiment.</p> <p><pre><code>#!/bin/sh \nscrapeghost https://www.ncleg.gov/Members/Biography/S/436  \\\n--schema \"{'first_name': 'str', 'last_name': 'str',\n        'photo_url': 'url', 'offices': [] }'\" \\\n--css div.card | python -m json.tool\n</code></pre> <pre><code>{\n\"first_name\": \"Gale\",\n\"last_name\": \"Adcock\",\n\"photo_url\": \"https://www.ncleg.gov/Members/MemberImage/S/436/Low\",\n\"offices\": [\n{\n\"type\": \"Mailing\",\n\"address\": \"16 West Jones Street, Rm. 1104, Raleigh, NC 27601\"\n},\n{\n\"type\": \"Office Phone\",\n\"phone\": \"(919) 715-3036\"\n}\n]\n}\n</code></pre></p> <p>See the CLI docs for more details.</p>"},{"location":"#features","title":"Features","text":"<p>The purpose of this library is to provide a convenient interface for exploring web scraping with GPT.</p> <p>While the bulk of the work is done by the GPT model, <code>scrapeghost</code> provides a number of features to make it easier to use.</p> <p>Python-based schema definition - Define the shape of the data you want to extract as any Python object, with as much or little detail as you want.</p> <p>Preprocessing</p> <ul> <li>HTML cleaning - Remove unnecessary HTML to reduce the size and cost of API requests.</li> <li>CSS and XPath selectors - Pre-filter HTML by writing a single CSS or XPath selector.</li> <li>Auto-splitting - Optionally split the HTML into multiple calls to the model, allowing for larger pages to be scraped.</li> </ul> <p>Postprocessing</p> <ul> <li>JSON validation - Ensure that the response is valid JSON.  (With the option to kick it back to GPT for fixes if it's not.)</li> <li>Schema validation - Go a step further, use a <code>pydantic</code> schema to validate the response.</li> <li>Hallucination check - Does the data in the response truly exist on the page?</li> </ul> <p>Cost Controls</p> <ul> <li>Scrapers keep running totals of how many tokens have been sent and received, so costs can be tracked.</li> <li>Support for automatic fallbacks (e.g. use cost-saving GPT-3.5-Turbo by default, fall back to GPT-4 if needed.)</li> <li>Allows setting a budget and stops the scraper if the budget is exceeded.</li> </ul>"},{"location":"LICENSE/","title":"Hippocratic License","text":"<p>HIPPOCRATIC LICENSE</p> <p>Version 3.0, October 2021</p> <p>https://firstdonoharm.dev/version/3/0/extr-ffd-law-mil-sv.md</p> <p>TERMS AND CONDITIONS</p> <p>TERMS AND CONDITIONS FOR USE, COPY, MODIFICATION, PREPARATION OF DERIVATIVE WORK, REPRODUCTION, AND DISTRIBUTION:</p> <p>1. DEFINITIONS:</p> <p>This section defines certain terms used throughout this license agreement.</p> <p>1.1. \u201cLicense\u201d means the terms and conditions, as stated herein, for use, copy, modification, preparation of derivative work, reproduction, and distribution of Software (as defined below).</p> <p>1.2. \u201cLicensor\u201d means the copyright and/or patent owner or entity authorized by the copyright and/or patent owner that is granting the License.</p> <p>1.3. \u201cLicensee\u201d means the individual or entity exercising permissions granted by this License, including the use, copy, modification, preparation of derivative work, reproduction, and distribution of Software (as defined below).</p> <p>1.4. \u201cSoftware\u201d means any copyrighted work, including but not limited to software code, authored by Licensor and made available under this License.</p> <p>1.5. \u201cSupply Chain\u201d means the sequence of processes involved in the production and/or distribution of a commodity, good, or service offered by the Licensee.</p> <p>1.6. \u201cSupply Chain Impacted Party\u201d or \u201cSupply Chain Impacted Parties\u201d means any person(s) directly impacted by any of Licensee\u2019s Supply Chain, including the practices of all persons or entities within the Supply Chain prior to a good or service reaching the Licensee.</p> <p>1.7. \u201cDuty of Care\u201d is defined by its use in tort law, delict law, and/or similar bodies of law closely related to tort and/or delict law, including without limitation, a requirement to act with the watchfulness, attention, caution, and prudence that a reasonable person in the same or similar circumstances would use towards any Supply Chain Impacted Party.</p> <p>1.8. \u201cWorker\u201d is defined to include any and all permanent, temporary, and agency workers, as well as piece-rate, salaried, hourly paid, legal young (minors), part-time, night, and migrant workers.</p> <p>2. INTELLECTUAL PROPERTY GRANTS:</p> <p>This section identifies intellectual property rights granted to a Licensee.</p> <p>2.1. Grant of Copyright License: Subject to the terms and conditions of this License, Licensor hereby grants to Licensee a worldwide, non-exclusive, no-charge, royalty-free copyright license to use, copy, modify, prepare derivative work, reproduce, or distribute the Software, Licensor authored modified software, or other work derived from the Software.</p> <p>2.2. Grant of Patent License: Subject to the terms and conditions of this License, Licensor hereby grants Licensee a worldwide, non-exclusive, no-charge, royalty-free patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer Software.</p> <p>3. ETHICAL STANDARDS:</p> <p>This section lists conditions the Licensee must comply with in order to have rights under this License.</p> <p>The rights granted to the Licensee by this License are expressly made subject to the Licensee\u2019s ongoing compliance with the following conditions:</p> <ul> <li>3.1. The Licensee SHALL NOT, whether directly or indirectly, through agents or assigns:  </li> <li>3.1.1. Infringe upon any person\u2019s right to life or security of person, engage in extrajudicial killings, or commit murder, without lawful cause (See Article 3, United Nations Universal Declaration of Human Rights; Article 6, International Covenant on Civil and Political Rights)  </li> <li>3.1.2. Hold any person in slavery, servitude, or forced labor (See Article 4, United Nations Universal Declaration of Human Rights; Article 8, International Covenant on Civil and Political Rights);  </li> <li>3.1.3. Contribute to the institution of slavery, slave trading, forced labor, or unlawful child labor (See Article 4, United Nations Universal Declaration of Human Rights; Article 8, International Covenant on Civil and Political Rights);  </li> <li>3.1.4. Torture or subject any person to cruel, inhumane, or degrading treatment or punishment (See Article 5, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Civil and Political Rights);  </li> <li>3.1.5. Discriminate on the basis of sex, gender, sexual orientation, race, ethnicity, nationality, religion, caste, age, medical disability or impairment, and/or any other like circumstances (See Article 7, United Nations Universal Declaration of Human Rights; Article 2, International Covenant on Economic, Social and Cultural Rights; Article 26, International Covenant on Civil and Political Rights);  </li> <li>3.1.6. Prevent any person from exercising his/her/their right to seek an effective remedy by a competent court or national tribunal (including domestic judicial systems, international courts, arbitration bodies, and other adjudicating bodies) for actions violating the fundamental rights granted to him/her/them by applicable constitutions, applicable laws, or by this License (See Article 8, United Nations Universal Declaration of Human Rights; Articles 9 and 14, International Covenant on Civil and Political Rights);  </li> <li>3.1.7. Subject any person to arbitrary arrest, detention, or exile (See Article 9, United Nations Universal Declaration of Human Rights; Article 9, International Covenant on Civil and Political Rights);  </li> <li>3.1.8. Subject any person to arbitrary interference with a person\u2019s privacy, family, home, or correspondence without the express written consent of the person (See Article 12, United Nations Universal Declaration of Human Rights; Article 17, International Covenant on Civil and Political Rights);  </li> <li>3.1.9. Arbitrarily deprive any person of his/her/their property (See Article 17, United Nations Universal Declaration of Human Rights);  </li> <li>3.1.10. Forcibly remove indigenous peoples from their lands or territories or take any action with the aim or effect of dispossessing indigenous peoples from their lands, territories, or resources, including without limitation the intellectual property or traditional knowledge of indigenous peoples, without the free, prior, and informed consent of indigenous peoples concerned (See Articles 8 and 10, United Nations Declaration on the Rights of Indigenous Peoples);  </li> <li>3.1.11. Fossil Fuel Divestment: Be an individual or entity, or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, on the FFI Solutions Carbon Underground 200 list;  </li> <li>3.1.12. Extractive Industries: Be an individual or entity, or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, that engages in fossil fuel or mineral exploration, extraction, development, or sale;  </li> <li>3.1.13. Mass Surveillance: Be a government agency or multinational corporation, or a representative, agent, affiliate, successor, attorney, or assign of a government or multinational corporation, which participates in mass surveillance programs;  </li> <li>3.1.14. Military Activities: Be an entity or a representative, agent, affiliate, successor, attorney, or assign of an entity which conducts military activities;  </li> <li>3.1.15. Law Enforcement: Be an individual or entity, or a or a representative, agent, affiliate, successor, attorney, or assign of an individual or entity, that provides good or services to, or otherwise enters into any commercial contracts with, any local, state, or federal law enforcement agency;  </li> <li>3.1.16. Interfere with Workers' free exercise of the right to organize and associate (See Article 20, United Nations Universal Declaration of Human Rights; C087 - Freedom of Association and Protection of the Right to Organise Convention, 1948 (No. 87), International Labour Organization; Article 8, International Covenant on Economic, Social and Cultural Rights); and  </li> <li>3.1.17. Harm the environment in a manner inconsistent with local, state, national, or international law.</li> <li>3.2. The Licensee SHALL:  </li> <li>3.2.1. Provide equal pay for equal work where the performance of such work requires equal skill, effort, and responsibility, and which are performed under similar working conditions, except where such payment is made pursuant to:  <ul> <li>3.2.1.1. A seniority system;</li> <li>3.2.1.2. A merit system;  </li> <li>3.2.1.3. A system which measures earnings by quantity or quality of production; or</li> <li>3.2.1.4. A differential based on any other factor other than sex, gender, sexual orientation, race, ethnicity, nationality, religion, caste, age, medical disability or impairment, and/or any other like circumstances (See 29 U.S.C.A. \u00a7 206(d)(1); Article 23, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Economic, Social and Cultural Rights; Article 26, International Covenant on Civil and Political Rights); and  </li> </ul> </li> <li>3.2.2. Allow for reasonable limitation of working hours and periodic holidays with pay (See Article 24, United Nations Universal Declaration of Human Rights; Article 7, International Covenant on Economic, Social and Cultural Rights).</li> </ul> <p>4. SUPPLY CHAIN IMPACTED PARTIES:</p> <p>This section identifies additional individuals or entities that a Licensee could harm as a result of violating the Ethical Standards section, the condition that the Licensee must voluntarily accept a Duty of Care for those individuals or entities, and the right to a private right of action that those individuals or entities possess as a result of violations of the Ethical Standards section.</p> <p>4.1. In addition to the above Ethical Standards, Licensee voluntarily accepts a Duty of Care for Supply Chain Impacted Parties of this License, including individuals and communities impacted by violations of the Ethical Standards. The Duty of Care is breached when a provision within the Ethical Standards section is violated by a Licensee, one of its successors or assigns, or by an individual or entity that exists within the Supply Chain prior to a good or service reaching the Licensee.</p> <p>4.2. Breaches of the Duty of Care, as stated within this section, shall create a private right of action, allowing any Supply Chain Impacted Party harmed by the Licensee to take legal action against the Licensee in accordance with applicable negligence laws, whether they be in tort law, delict law, and/or similar bodies of law closely related to tort and/or delict law, regardless if Licensee is directly responsible for the harms suffered by a Supply Chain Impacted Party. Nothing in this section shall be interpreted to include acts committed by individuals outside of the scope of his/her/their employment.</p> <p>5. NOTICE: This section explains when a Licensee must notify others of the License.</p> <p>5.1. Distribution of Notice: Licensee must ensure that everyone who receives a copy of or uses any part of Software from Licensee, with or without changes, also receives the License and the copyright notice included with Software (and if included by the Licensor, patent, trademark, and attribution notice). Licensee must ensure that License is prominently displayed so that any individual or entity seeking to download, copy, use, or otherwise receive any part of Software from Licensee is notified of this License and its terms and conditions. Licensee must cause any modified versions of the Software to carry prominent notices stating that Licensee changed the Software.</p> <p>5.2. Modified Software: Licensee is free to create modifications of the Software and distribute only the modified portion created by Licensee, however, any derivative work stemming from the Software or its code must be distributed pursuant to this License, including this Notice provision.</p> <p>5.3. Recipients as Licensees: Any individual or entity that uses, copies, modifies, reproduces, distributes, or prepares derivative work based upon the Software, all or part of the Software\u2019s code, or a derivative work developed by using the Software, including a portion of its code, is a Licensee as defined above and is subject to the terms and conditions of this License.</p> <p>6. REPRESENTATIONS AND WARRANTIES:</p> <p>6.1. Disclaimer of Warranty: TO THE FULL EXTENT ALLOWED BY LAW, THIS SOFTWARE COMES \u201cAS IS,\u201d WITHOUT ANY WARRANTY, EXPRESS OR IMPLIED, AND LICENSOR SHALL NOT BE LIABLE TO ANY PERSON OR ENTITY FOR ANY DAMAGES OR OTHER LIABILITY ARISING FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THIS LICENSE, UNDER ANY LEGAL CLAIM.</p> <p>6.2. Limitation of Liability: LICENSEE SHALL HOLD LICENSOR HARMLESS AGAINST ANY AND ALL CLAIMS, DEBTS, DUES, LIABILITIES, LIENS, CAUSES OF ACTION, DEMANDS, OBLIGATIONS, DISPUTES, DAMAGES, LOSSES, EXPENSES, ATTORNEYS' FEES, COSTS, LIABILITIES, AND ALL OTHER CLAIMS OF EVERY KIND AND NATURE WHATSOEVER, WHETHER KNOWN OR UNKNOWN, ANTICIPATED OR UNANTICIPATED, FORESEEN OR UNFORESEEN, ACCRUED OR UNACCRUED, DISCLOSED OR UNDISCLOSED, ARISING OUT OF OR RELATING TO LICENSEE\u2019S USE OF THE SOFTWARE. NOTHING IN THIS SECTION SHOULD BE INTERPRETED TO REQUIRE LICENSEE TO INDEMNIFY LICENSOR, NOR REQUIRE LICENSOR TO INDEMNIFY LICENSEE.</p> <p>7. TERMINATION</p> <p>7.1. Violations of Ethical Standards or Breaching Duty of Care: If Licensee violates the Ethical Standards section or Licensee, or any other person or entity within the Supply Chain prior to a good or service reaching the Licensee, breaches its Duty of Care to Supply Chain Impacted Parties, Licensee must remedy the violation or harm caused by Licensee within 30 days of being notified of the violation or harm. If Licensee fails to remedy the violation or harm within 30 days, all rights in the Software granted to Licensee by License will be null and void as between Licensor and Licensee.</p> <p>7.2. Failure of Notice: If any person or entity notifies Licensee in writing that Licensee has not complied with the Notice section of this License, Licensee can keep this License by taking all practical steps to comply within 30 days after the notice of noncompliance. If Licensee does not do so, Licensee\u2019s License (and all rights licensed hereunder) will end immediately.</p> <p>7.3. Judicial Findings: In the event Licensee is found by a civil, criminal, administrative, or other court of competent jurisdiction, or some other adjudicating body with legal authority, to have committed actions which are in violation of the Ethical Standards or Supply Chain Impacted Party sections of this License, all rights granted to Licensee by this License will terminate immediately.</p> <p>7.4. Patent Litigation: If Licensee institutes patent litigation against any entity (including a cross-claim or counterclaim in a suit) alleging that the Software, all or part of the Software\u2019s code, or a derivative work developed using the Software, including a portion of its code, constitutes direct or contributory patent infringement, then any patent license, along with all other rights, granted to Licensee under this License will terminate as of the date such litigation is filed.</p> <p>7.5. Additional Remedies: Termination of the License by failing to remedy harms in no way prevents Licensor or Supply Chain Impacted Party from seeking appropriate remedies at law or in equity.</p> <p>8. MISCELLANEOUS:</p> <p>8.1. Conditions: Sections 3, 4.1, 5.1, 5.2, 7.1, 7.2, 7.3, and 7.4 are conditions of the rights granted to Licensee in the License.</p> <p>8.2. Equitable Relief: Licensor and any Supply Chain Impacted Party shall be entitled to equitable relief, including injunctive relief or specific performance of the terms hereof, in addition to any other remedy to which they are entitled at law or in equity.</p> <p>8.3. Severability: If any term or provision of this License is determined to be invalid, illegal, or unenforceable by a court of competent jurisdiction, any such determination of invalidity, illegality, or unenforceability shall not affect any other term or provision of this License or invalidate or render unenforceable such term or provision in any other jurisdiction. If the determination of invalidity, illegality, or unenforceability by a court of competent jurisdiction pertains to the terms or provisions contained in the Ethical Standards section of this License, all rights in the Software granted to Licensee shall be deemed null and void as between Licensor and Licensee.</p> <p>8.4. Section Titles: Section titles are solely written for organizational purposes and should not be used to interpret the language within each section.</p> <p>8.5. Citations: Citations are solely written to provide context for the source of the provisions in the Ethical Standards.</p> <p>8.6. Section Summaries: Some sections have a brief italicized description which is provided for the sole purpose of briefly describing the section and should not be used to interpret the terms of the License.</p> <p>8.7. Entire License: This is the entire License between the Licensor and Licensee with respect to the claims released herein and that the consideration stated herein is the only consideration or compensation to be paid or exchanged between them for this License. This License cannot be modified or amended except in a writing signed by Licensor and Licensee.</p> <p>8.8. Successors and Assigns: This License shall be binding upon and inure to the benefit of the Licensor\u2019s and Licensee\u2019s respective heirs, successors, and assigns.</p>"},{"location":"api/","title":"API Reference","text":"<p>Warning</p> <p>Due to the rapidly evolving API, this page may be slightly out of date. </p>"},{"location":"api/#schemascraper","title":"<code>SchemaScraper</code>","text":"<p>The <code>SchemaScraper</code> class is the main interface to the API.</p> <p>It has one required parameter:</p> <ul> <li><code>schema</code> - A dictionary describing the shape of the data you wish to extract.</li> </ul> <p>And the following optional parameters:</p> <ul> <li><code>models</code> - list[str] - A list of models to use, in order of preference.  Defaults to <code>[\"gpt-3.5-turbo\", \"gpt-4\"]</code>.  </li> <li><code>model_params</code> - dict - A dictionary of parameters to pass to the underlying GPT model.  (See OpenAI docs for details.)</li> <li><code>max_cost</code> -  float (dollars) - The maximum total cost of calls made using this scraper. This is set to 1 ($1.00) by default to avoid large unexpected charges.</li> <li><code>extra_instructions</code> - list[str] - Additional instructions to pass to the GPT model as a system prompt.</li> <li><code>extra_preprocessors</code> - list - A list of preprocessors to run on the HTML before sending it to the API.  This is in addition to the default preprocessors.</li> <li><code>postprocessors</code> - list - A list of postprocessors to run on the results before returning them.  If provided, this will override the default postprocessors.</li> <li><code>auto_split_length</code> - int - If set, the scraper will split the page into multiple calls, each of this length. See auto-splitting for details.</li> </ul>"},{"location":"api/#scrape","title":"<code>scrape</code>","text":"<p>The <code>scrape</code> method of a <code>SchemaScraper</code> is used to scrape a page.</p> <pre><code>scraper = SchemaScraper(schema)\nscraper.scrape(\"https://example.com\")\n</code></pre> <ul> <li><code>url_or_html</code> - The first parameter should be a URL or HTML string to scrape.</li> <li><code>extra_preprocessors</code> - A list of preprocessors to run on the HTML before sending it to the API.</li> </ul> <p>It is also possible to call the scraper directly, which is equivalent to calling <code>scrape</code>:</p> <pre><code>scraper = SchemaScraper(schema)\nscraper(\"https://example.com\")\n# same as writing\nscraper.scrape(\"https://example.com\")\n</code></pre>"},{"location":"api/#exceptions","title":"Exceptions","text":"<p>The following exceptions can be raised by the scraper:</p> <p>(all are subclasses of <code>ScrapeghostError</code>)</p>"},{"location":"api/#maxcostexceeded","title":"<code>MaxCostExceeded</code>","text":"<p>The maximum cost of the scraper has been exceeded.</p> <p>Raise the <code>max_cost</code> parameter to allow more calls to be made.</p>"},{"location":"api/#preprocessorerror","title":"<code>PreprocessorError</code>","text":"<p>A preprocessor encountered an error (such as returning an empty list of nodes).</p>"},{"location":"api/#toomanytokens","title":"<code>TooManyTokens</code>","text":"<p>Raised when the number of tokens being sent exceeds the maximum allowed.</p> <p>This indicates that the HTML is too large to be processed by the API.</p> <p>Tip</p> <p>Consider using the <code>css</code> or <code>xpath</code> selectors to reduce the number of tokens being sent, or use the <code>auto_split_length</code> parameter to split the request into multiple requests if necessary.</p>"},{"location":"api/#badstop","title":"<code>BadStop</code>","text":"<p>Indicates that OpenAI ran out of space before the stop token was reached.</p> <p>Tip</p> <p>OpenAI considers both the input and the response tokens when determining if the token limit has been exceeded.</p> <p>If you are using <code>auto_split_length</code>, consider decreasing the value to leave more space for responses.</p>"},{"location":"api/#invalidjson","title":"<code>InvalidJSON</code>","text":"<p>Indicates that the JSON returned by the API is invalid.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#041-2023-03-24","title":"0.4.1 - 2023-03-24","text":"<ul> <li>Fix bug with HallucinationCheck.</li> </ul>"},{"location":"changelog/#040-2023-03-24","title":"0.4.0 - 2023-03-24","text":"<ul> <li>New configurable pre- and post-processing pipelines for customizing behavior.</li> <li>Addition of <code>ScrapeResult</code> object to hold results of scraping along with metadata.</li> <li>Support for <code>pydantic</code> models as schemas and for validation.</li> <li>\"Hallucination\" check to ensure that the data in the response truly exists on the page.</li> <li>Use post-processing pipeline to \"nudge\" JSON errors to a better result.</li> <li>Now fully type-annotated.</li> <li>Another big refactor, separation of API calls and scraping logic.</li> <li>Finally, a ghost logo reminiscent of library's namesake.</li> </ul>"},{"location":"changelog/#030-2023-03-20","title":"0.3.0 - 2023-03-20","text":"<ul> <li>Add tests, docs, and complete examples!</li> <li>Add preprocessors to <code>SchemaScraper</code> to allow for uniform interface for cleaning &amp; selecting HTML.</li> <li>Use <code>tiktoken</code> for accurate token counts.</li> <li>New <code>cost_estimate</code> utility function.</li> <li>Cost is now tracked on a per-scraper basis (see the <code>total_cost</code> attribute on <code>SchemaScraper</code> objects).</li> <li><code>SchemaScraper</code> now takes a <code>max_cost</code> parameter to limit the total cost of a scraper.</li> <li>Prompt improvements, list mode simplification.</li> </ul>"},{"location":"changelog/#020-2023-03-18","title":"0.2.0 - 2023-03-18","text":"<ul> <li>Add list mode, auto-splitting, and pagination support.</li> <li>Improve <code>xpath</code> and <code>css</code> handling.</li> <li>Improve prompt for GPT 3.5.</li> <li>Make it possible to alter parameters when calling scrape.</li> <li>Logging &amp; error handling.</li> <li>Command line interface.</li> <li>See blog post for details: https://jamesturk.net/posts/scraping-with-gpt-part-2/</li> </ul>"},{"location":"changelog/#010-2023-03-17","title":"0.1.0 - 2023-03-17","text":"<ul> <li>Initial experiment, see blog post for more: https://jamesturk.net/posts/scraping-with-gpt-4/</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>scrapeghost offers a command line interface which is particularly useful for experimentation.</p> <p>It is also possible to use as a step in a data pipeline.</p>"},{"location":"cli/#configuration","title":"Configuration","text":"<p>In order to use the CLI, the <code>OPENAI_API_KEY</code> environment variable must be set.</p>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>scrapeghost --help\n Usage: scrapeghost [OPTIONS] URL                                                                                                                                            \n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    url      TEXT  [default: None] [required]                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --xpath                         TEXT     XPath selector to narrow the scrape [default: None]        \u2502\n\u2502 --css                           TEXT     CSS selector to narrow the scrape [default: None]          \u2502\n\u2502 --schema                        TEXT     Schema to use for scraping [default: None]                 \u2502\n\u2502 --schema-file                   PATH     Path to schema.json file [default: None]                   \u2502\n\u2502 --gpt4             --no-gpt4             Use GPT-4 instead of GPT-3.5-turbo [default: no-gpt4]      \u2502\n\u2502 --verbose      -v               INTEGER  Verbosity level 0-2 [default: 0]                           \u2502\n\u2502 --help                                   Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement:</p> <ul> <li>James Turk: dev@jamesturk.net</li> </ul> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"faq/","title":"FAQ","text":"<p>Mostly questions I've been frequently asking myself.</p>"},{"location":"faq/#is-this-practical-or-just-a-toy","title":"Is this practical? Or just a toy?","text":"<p>When I started the project I mostly assumed it was a toy. But I've been surprised by the results.</p> <p>After my initial GPT-4 experiments, Simon Willison asked how well it'd work on GPT-3.5-turbo. I hadn't realized the significant price difference, and without switching to 3.5-turbo, I'd probably have decided it was too expensive to be practical.</p> <p>Once I realized 3.5-turbo was an option, I was able to spend a lot more time tinkering with the prompt and token reduction.  It also got me thinking more about what kind of tooling you'd want around something like this if you were going to actually use it.</p>"},{"location":"faq/#why-would-i-use-this-instead-of-a-traditional-scraper","title":"Why would I use this instead of a traditional scraper?","text":"<p>It is definitely great for quick prototypes. With the CLI tool, you can try a scrape in a single command without writing a line of code. This means you don't need to sink a bunch of time into deciding if it's worth it or not.</p> <p>Or, imagine a scraper that needs to run infrequently on a page that is likely to break in subtle ways between scrapes. A CSS/XPath-based scraper will often be broken in small ways between the first run and another run months later, there's a decent chance that those changes won't break a GPT-based scraper.</p> <p>It is also quite good at dealing with unstructured text. A list of items in a sentence can be hard to handle with a traditional scraper, but GPT handles many of these cases without much fuss.</p>"},{"location":"faq/#what-are-the-disadvantages","title":"What are the disadvantages?","text":"<ul> <li>It is terrible at pages that are large lists (like a directory), they need to be broken into multiple chunks and the API calls can be expensive in terms of time and money.</li> <li>It is opaque.  When it fails, it can be hard to tell why.</li> <li>If the page is dynamic, this approach won't work at all.  It requires all of the content to be available in the HTML.</li> <li>It is slow.  A single request can take over a minute if OpenAI is slow to respond.</li> <li>Right now, it only works with OpenAI, that means you'll be dependent on their pricing and availability. It also means you need to be comfortable sending your data to a third party.</li> </ul>"},{"location":"faq/#why-not-use-a-different-model","title":"Why not use a different model?","text":"<p>I'm open to trying other models if you have suggestions.  (See https://github.com/jamesturk/scrapeghost/issues/18)</p>"},{"location":"faq/#what-can-i-do-if-a-page-is-too-big","title":"What can I do if a page is too big?","text":"<p>Try the following:</p> <ol> <li> <p>Provide a CSS or XPath selector to limit the scope of the page.</p> </li> <li> <p>Pre-process the HTML. Trim tags or entire sections you don't need.  (You can use the preprocessing pipeline to help with this.)</p> </li> <li> <p>Finally, you can use the <code>auto_split_length</code> parameter to split the page into smaller chunks.  This only works for list-type pages, and requires a good choice of selector to split the page up.</p> </li> </ol>"},{"location":"faq/#why-not-ask-the-scraper-to-write-css-xpath-selectors","title":"Why not ask the scraper to write CSS / XPath selectors?","text":"<p>While it'd seem like this would perform better, there are a few practical challenges standing in the way right now.</p> <ul> <li>Writing a robust CSS/XPath selector that'd run against a whole set of pages would require passing a lot of context to the model. The token limit is already the major limitation.</li> <li>The current solution does not require any changes when a page changes.  A selector-based model would require retraining every time a page changes as well as a means to detect such changes.</li> <li>For some data, selectors alone are not enough. The current model can easily extract all of the addresses from a page and break them into city/state/etc. A selector-based model would not be able to do this.</li> </ul> <p>I do think there is room for hybrid approaches, and I plan to continue to explore them.</p>"},{"location":"faq/#does-the-model-hallucinate-data","title":"Does the model \"hallucinate\" data?","text":"<p>It is possible, but in practice hasn't been observed as a major problem yet.</p> <p>Because the temperature is zero, the output is fully deterministic and seems less likely to hallucinate data.</p> <p>The <code>HallucinationChecker</code> class can be used to detect data that appears in the response that doesn't appear on the page. This approach could be improved, but I haven't seen hallucination as a major problem yet.  (If you have examples, please open an issue!)</p>"},{"location":"faq/#how-much-did-you-spend-developing-this","title":"How much did you spend developing this?","text":"<p>So far, about $40 on API calls, switching to GPT-3.5 as the default made a big difference.</p> <p>My most expensive call was a paginated GPT-4 call that cost $2.20.  I decided to add the cost-limiting features after that.</p>"},{"location":"faq/#whats-with-the-license","title":"What's with the license?","text":"<p>I'm still working on figuring this out.</p> <p>For now, if you're working in a commercial setting and the license scares you away, that's fine.</p> <p>If you really want to, you can contact me and we can work something out.</p>"},{"location":"openai/","title":"OpenAI / GPT","text":"<p>This section assumes you are mostly unfamiliar with the OpenAI API and aims to provide a high-level overview of how it works in relation to this library. </p>"},{"location":"openai/#api-keys","title":"API Keys","text":""},{"location":"openai/#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the OpenAI API you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>It's strongly recommended that you set a usage limit on your API key to avoid accidentally running up a large bill.</p> <p>https://platform.openai.com/account/billing/limits lets you set usage limits to avoid unpleasant surprises.</p>"},{"location":"openai/#using-your-key","title":"Using your key","text":"<p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>$ export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Be careful not to expose this key to the public by checking it into a public repository.</p>"},{"location":"openai/#costs","title":"Costs","text":"<p>The OpenAI API is considerably expensive.</p> <p>The cost of a call varies based on the model used and the size of the input.</p> <p>The cost estimates provided by this library are based on the OpenAI pricing page and not guaranteed to be accurate.</p> <p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12 <p>Example: A 3,000 token page that returns 1,000 tokens of JSON will cost $0.008 with GPT-3-Turbo, but $0.15 with GPT-4.</p> <p>(See OpenAI pricing page for latest info.)</p>"},{"location":"openai/#tokens","title":"Tokens","text":"<p>OpenAI encodes text using a tokenizer, which converts words to integers.</p> <p>You'll see that billing is based on the number of tokens used.  A token is approximately 3 characters, so 3000 characters of HTML will roughly correspond to 1000 tokens.</p> <p>Additionally, the GPT-3-Turbo model is limited to 4096 tokens.  GPT-4 is limited to 8192 tokens.  (A 32k model has been announced, but is not yet widely available.)</p> <p>Various features in the library will help you avoid running into token limits, but it is still very common to exceed them in practice.</p> <p>If your pages exceed these limits, you'll need to focus on improving your selectors so that only the required data is sent to the underlying models.</p>"},{"location":"openai/#prompts","title":"Prompts","text":"<p>The OpenAI API provides a chat-like interface, where there are three roles: system, user, and assistant.  The system commands provide guidance to the assistant on how it should perform its tasks.  The user provides a query to the assistant, which is then answered.</p> <p>In practice, this results in a prompt that looks like something this:</p> <p>System: For the given HTML, convert to a list of JSON objects matching this schema: <code>{\"name\": \"string\", \"age\": \"number\"}</code></p> <p>System: Be sure to provide valid JSON that is not truncated and contains no extra fields beyond those in the schema.</p> <p>User: <code>&lt;html&gt;&lt;div&gt;&lt;h2&gt;Joe&lt;/h2&gt;&lt;span&gt;Age: 42&lt;/span&gt;&lt;/div&gt;&lt;/html&gt;</code></p> <p>Assistant: <code>{\"name\": \"Joe\", \"age\": 42}</code></p> <p>It is possible to adjust the system commands the library sends, but the goal is to provide a simple default prompt that works well for most use cases.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial will show you how to use <code>scrapeghost</code> to build a web scraper without writing page-specific code.</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorial/#install-scrapeghost","title":"Install <code>scrapeghost</code>","text":"<p>You'll need to install <code>scrapeghost</code>. You can do this with <code>pip</code>, <code>poetry</code>, or your favorite Python package manager.</p>"},{"location":"tutorial/#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the OpenAI API you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>It's strongly recommended that you set a usage limit on your API key to avoid accidentally running up a large bill.</p> <p>https://platform.openai.com/account/billing/limits lets you set usage limits to avoid unpleasant surprises.</p>"},{"location":"tutorial/#using-your-key","title":"Using your key","text":"<p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>$ export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Be careful not to expose this key to the public by checking it into a public repository.</p>"},{"location":"tutorial/#writing-a-scraper","title":"Writing a Scraper","text":"<p>The goal of our scraper is going to be to get a list of all of the episodes of the podcast Comedy Bang Bang.</p> <p>To do this, we'll need two kinds of scrapers: one to get a list of all of the episodes, and one to get the details of each episode.</p>"},{"location":"tutorial/#getting-episode-details","title":"Getting Episode Details","text":"<p>At the time of writing, the most recent episode of Comedy Bang Bang is Episode 800, Operation Golden Orb.</p> <p>The URL for this episode is https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb.</p> <p>Let's say we want to build a scraper that finds out each episode's title, episode number, and release date.</p> <p>We can do this by creating a <code>SchemaScraper</code> object and passing it a schema.</p> <pre><code>from scrapeghost import SchemaScraper\nfrom pprint import pprint\n\nurl = \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"str\",\n}\n\nepisode_scraper = SchemaScraper(schema)\n\nresponse = episode_scraper(url)\npprint(response.data)\nprint(f\"Total Cost: ${response.total_cost:.3f}\")\n</code></pre> <p>There is no predefined way to define a schema, but a dictionary resembling the data you want to scrape where the keys are the names of the fields you want to scrape and the values are the types of the fields is a good place to start.</p> <p>Once you have an instance of <code>SchemaScraper</code> you can use it to scrape a specific page by passing it a URL (or HTML if you prefer/need to fetch the data another way).</p> <p>Running our code gives an error though:</p> <pre><code>scrapeghost.scrapers.TooManyTokens: HTML is 9710 tokens, max for gpt-3.5-turbo is 4096\n</code></pre> <p>This means that the content length is too long, we'll need to reduce our token count in order to make this work.</p>"},{"location":"tutorial/#what-are-tokens","title":"What Are Tokens?","text":"<p>If you haven't used OpenAI's APIs before, you may not be aware of the token limits.  Every request has a limit on the number of tokens it can use. For GPT-4 this is 8,192 tokens. For GPT-3.5-Turbo it is 4,096.  (A token is about three characters.)</p> <p>You are also billed per token, so even if you're under the limit, fewer tokens means cheaper API calls.</p> <p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12 <p>Example: A 3,000 token page that returns 1,000 tokens of JSON will cost $0.008 with GPT-3-Turbo, but $0.15 with GPT-4.</p> <p>(See OpenAI pricing page for latest info.)</p> <p>Ideally, we'd only pass the relevant parts of the page to OpenAI. It shouldn't need anything outside of the HTML <code>&lt;body&gt;</code>, anything in comments, script tags, etc.</p> <p>(For more details on how this library interacts with OpenAI's API, see the OpenAI API page.)</p>"},{"location":"tutorial/#preprocessors","title":"Preprocessors","text":"<p>To help with all this, <code>scrapeghost</code> provides a way to preprocess the HTML before it is sent to OpenAI. This is done by passing a list of preprocessor callables to the <code>SchemaScraper</code> constructor.</p> <p>Info</p> <p>A <code>CleanHTML</code> preprocessor is included by default. This removes HTML comments, script tags, and style tags.</p> <p>If you visit the page https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb viewing the source will reveal that all of the interesting content is in an element <code>&lt;div id=\"content\" class=\"page-content\"&gt;</code>.</p> <p>Just as we might if we were writing a real scraper, we'll write a CSS selector to grab this element, <code>div.page-content</code> will do. The <code>CSS</code> preprocessor will use this selector to extract the content of the element.</p> <pre><code>from scrapeghost import SchemaScraper, CSS\nfrom pprint import pprint\n\nurl = \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"str\",\n}\n\nepisode_scraper = SchemaScraper(\n    schema,\n# can pass preprocessor to constructor or at scrape time\nextra_preprocessors=[CSS(\"div.page-content\")],\n)\n\nresponse = episode_scraper(url)\npprint(response.data)\nprint(f\"Total Cost: ${response.total_cost:.3f}\")\n</code></pre> <p>Now, a call to our scraper will only pass the content of the <code>&lt;div&gt;</code> to OpenAI. We get the following output:</p> <pre><code>2023-03-24 19:18:57 [info     ] API request                    html_tokens=1332 model=gpt-3.5-turbo\n2023-03-24 19:18:59 [info     ] API response                   completion_tokens=33 cost=0.00291 duration=2.3073980808258057 finish_reason=stop prompt_tokens=1422\n{'episode_number': 800,\n 'release_date': 'March 12, 2023',\n 'title': 'Operation Golden Orb'}\nTotal Cost: $0.003\n</code></pre> <p>We can see from the logging output that the content length is much shorter now and we get the data we were hoping for.</p> <p>All for less than a penny!</p> <p>Tip</p> <p>Even when the page fits under the token limit, it is still a good idea to pass a selector to limit the amount of content that OpenAI has to process.</p> <p>Fewer tokens means faster responses and cheaper API calls. It should also get you better results.</p>"},{"location":"tutorial/#enhancing-the-schema","title":"Enhancing the Schema","text":"<p>That was easy! Let's enhance our schema to include the list of guests as well as requesting the dates in a particular format.</p> <pre><code>from scrapeghost import SchemaScraper, CSS\nfrom pprint import pprint\n\nurl = \"https://comedybangbang.fandom.com/wiki/Operation_Golden_Orb\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n\"release_date\": \"YYYY-MM-DD\",\n\"guests\": [{\"name\": \"str\"}],\n}\n\nepisode_scraper = SchemaScraper(\n    schema,\n    # can pass preprocessor to constructor or at scrape time\n    extra_preprocessors=[CSS(\"div.page-content\")],\n)\n\nresponse = episode_scraper(url)\npprint(response.data)\nprint(f\"Total Cost: ${response.total_cost:.3f}\")\n</code></pre> <p>Just two small changes, but now we get the following output:</p> <pre><code>2023-03-24 19:19:00 [info     ] API request                    html_tokens=1332 model=gpt-3.5-turbo\n2023-03-24 19:19:05 [info     ] API response                   completion_tokens=83 cost=0.003036 duration=4.687386989593506 finish_reason=stop prompt_tokens=1435\n{'episode_number': 800,\n 'guests': [{'name': 'Jason Mantzoukas'},\n            {'name': 'Andy Daly'},\n            {'name': 'Paul F. Tompkins'}],\n 'release_date': '2023-03-12',\n 'title': 'Operation Golden Orb'}\nTotal Cost: $0.003\n</code></pre> <p>Let's try it on a different episode, from the beginning of the series.</p> <p><pre><code>episode_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Welcome_to_Comedy_Bang_Bang\",\n).data\n</code></pre> <pre><code>{'episode_number': 1,\n 'guests': [{'name': 'Rob Huebel'},\n            {'name': 'Tom Lennon'},\n            {'name': 'Doug Benson'}],\n 'release_date': '2009-05-01',\n 'title': 'Welcome to Comedy Bang Bang'}\n</code></pre></p> <p>Not bad!</p>"},{"location":"tutorial/#dealing-with-page-structure-changes","title":"Dealing With Page Structure Changes","text":"<p>If you've maintained a scraper for any amount of time you know that the biggest burden is dealing with changes to the structure of the pages you're scraping.</p> <p>To simulate this, let's say we instead wanted to get the same information from a different page: https://www.earwolf.com/episode/operation-golden-orb/</p> <p>This page has a completely different layout. We will need to change our CSS selector:</p> <p><pre><code>from scrapeghost import SchemaScraper, CSS\nfrom pprint import pprint\n\nurl = \"https://www.earwolf.com/episode/operation-golden-orb/\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"YYYY-MM-DD\",\n    \"guests\": [{\"name\": \"str\"}],\n}\n\nepisode_scraper = SchemaScraper(\n    schema,\nextra_preprocessors=[CSS(\".hero-episode\")],\n)\n\nresponse = episode_scraper(url)\npprint(response.data)\nprint(f\"Total Cost: ${response.total_cost:.3f}\")\n</code></pre> <pre><code>2023-03-24 19:19:08 [info     ] API request                    html_tokens=2988 model=gpt-3.5-turbo\n2023-03-24 19:19:13 [info     ] API response                   completion_tokens=88 cost=0.006358000000000001 duration=5.002557992935181 finish_reason=stop prompt_tokens=3091\n{'episode_number': 800,\n 'guests': [{'name': 'Jason Mantzoukas'},\n            {'name': 'Andy Daly'},\n            {'name': 'Paul F. Tompkins'}],\n 'release_date': '2023-03-12',\n 'title': 'EP. 800 \u2014 Operation Golden Orb'}\nTotal Cost: $0.006\n</code></pre></p> <p>Completely different HTML, one CSS selector change.</p>"},{"location":"tutorial/#extra-instructions","title":"Extra Instructions","text":"<p>You may notice that the <code>title</code> changed. The second source includes the episode number in the title, but the first source does not.</p> <p>You could deal with this with a bit of clean up, but you have another option at your disposal. You can give the underlying model additional instructions to modify the behavior.</p> <p><pre><code>from scrapeghost import SchemaScraper, CSS\nfrom pprint import pprint\n\nurl = \"https://www.earwolf.com/episode/operation-golden-orb/\"\nschema = {\n    \"title\": \"str\",\n    \"episode_number\": \"int\",\n    \"release_date\": \"YYYY-MM-DD\",\n    \"guests\": [{\"name\": \"str\"}],\n}\n\nepisode_scraper = SchemaScraper(\n    schema,\n    extra_preprocessors=[CSS(\".hero-episode\")],\n    extra_instructions=[\n\"Do not include the episode number in the title.\",\n],\n)\n\nresponse = episode_scraper(url)\npprint(response.data)\nprint(f\"Total Cost: ${response.total_cost:.3f}\")\n</code></pre> <pre><code>2023-03-24 19:19:14 [info     ] API request                    html_tokens=2988 model=gpt-3.5-turbo\n2023-03-24 19:19:18 [info     ] API response                   completion_tokens=83 cost=0.006378 duration=4.542723894119263 finish_reason=stop prompt_tokens=3106\n{'episode_number': 800,\n 'guests': [{'name': 'Jason Mantzoukas'},\n            {'name': 'Andy Daly'},\n            {'name': 'Paul F. Tompkins'}],\n 'release_date': '2023-03-12',\n 'title': 'Operation Golden Orb'}\nTotal Cost: $0.006\n</code></pre></p> <p>At this point, you may be wondering if you'll ever need to write a web scraper again. </p> <p>So to temper that, let's take a look at something that is a bit more difficult for <code>scrapeghost</code> to handle.</p>"},{"location":"tutorial/#getting-a-list-of-episodes","title":"Getting a List of Episodes","text":"<p>Now that we have a scraper that can get the details of each episode, we want a scraper that can get a list of all of the episode URLs.</p> <p>https://comedybangbang.fandom.com/wiki/Category:Episodes has a link to each of the episodes, perhaps we can just scrape that page?</p> <p><pre><code>from scrapeghost import SchemaScraper\n\nepisode_list_scraper = SchemaScraper({\"episode_urls\": [\"str\"]})\nepisode_list_scraper(\"https://comedybangbang.fandom.com/wiki/Category:Episodes\")\n</code></pre> <pre><code>scrapeghost.scrapers.TooManyTokens: HTML is 292918 tokens, max for gpt-3.5-turbo is 4096\n</code></pre></p> <p>Yikes, nearly 300k tokens! This is a huge page.</p> <p>We can try again with a CSS selector, but this time we'll try to get a selector for each individual item.</p> <p>If you have go this far, you may want to just extract links using <code>lxml.html</code> or <code>BeautifulSoup</code> instead.</p> <p>But let's imagine that for some reason you don't want to, perhaps this is a one-off project and even a relatively expensive request is worth it.</p> <p><code>SchemaScraper</code> has a few options that will help, we'll change our scraper to use <code>auto_split_length</code>.</p> <pre><code>from scrapeghost import SchemaScraper, CSS\n\nepisode_list_scraper = SchemaScraper(\n    \"url\",\n    auto_split_length=2000,\n    extra_preprocessors=[CSS(\".mw-parser-output a[class!='image link-internal']\")],\n)\nresponse = episode_list_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Category:Episodes\"\n)\n\nepisode_urls = response.data\nprint(episode_urls[:3])\nprint(episode_urls[-3:])\nprint(\"total:\", len(episode_urls))\nprint(f\"Total Cost: ${response.total_cost:.3f}\")\n</code></pre> <p>We set the <code>auto_split_length</code> to 2000. This is the maximum number of tokens that will be passed to OpenAI in a single request.</p> <p>Setting <code>auto_split_length</code> alters the prompt and response format so that instead of returning a single JSON object, it returns a list of objects where each should match your provided <code>schema</code>.</p> <p>Because of this, we alter the <code>schema</code> to just be a single string because we're only interested in the URL.</p> <p>It's a good idea to set this to about half the token limit, since the response counts against the token limit as well.</p> <p>This winds up needing to make over twenty requests, but can get there.</p> <pre><code>        *relevant log lines shown for clarity*\n2023-03-24 19:25:53 [debug    ] got HTML                       length=1424892 url=https://comedybangbang.fandom.com/wiki/Category:Episodes\n2023-03-24 19:25:53 [debug    ] preprocessor                   from_nodes=1 name=CleanHTML nodes=1\n2023-03-24 19:25:53 [debug    ] preprocessor                   from_nodes=1 name=CSS(.mw-parser-output a[class!='image link-internal']) nodes=857\n2023-03-24 19:25:53 [debug    ] chunked tags                   num=20 sizes=[1971, 1994, 1986, 1976, 1978, 1990, 1993, 1974, 1995, 1983, 1975, 1979, 1967, 1953, 1971, 1973, 1987, 1960, 1966, 682]\n2023-03-24 19:25:53 [info     ] API request                    html_tokens=1971 model=gpt-3.5-turbo\n2023-03-24 19:27:38 [info     ] API response                   completion_tokens=2053 cost=0.008194 duration=104.66404676437378 finish_reason=length prompt_tokens=2044\n2023-03-24 19:31:28 [warning  ] retry                          model=gpt-4 wait=30\n2023-03-24 19:36:34 [warning  ] API request failed             attempts=1 model=gpt-3.5-turbo\n2023-03-24 19:36:34 [warning  ] retry                          model=gpt-4 wait=30\n2023-03-24 19:41:53 [warning  ] API request failed             attempts=1 model=gpt-3.5-turbo\n2023-03-24 19:41:53 [warning  ] retry                          model=gpt-4 wait=30\nscrapeghost.errors.MaxCostExceeded: Total cost 1.04 exceeds max cost 1.00\n</code></pre> <p>As you can see, a couple of requests had to fall back to GPT-4, which raised the cost.</p> <p>As a safeguard, the maximum cost for a single scrape is configured to $1 by default. If you want to change this, you can set the <code>max_cost</code> parameter.</p> <p>One option is to lower the <code>split_token</code> a bit further. Many more requests means it takes even longer, but if you can stick to GPT-3.5-Turbo it was possible to get a scrape to complete for $0.13.</p> <p>But as promised, this is something that <code>scrapeghost</code> isn't currently very good at.</p> <p>If you do want to see the pieces put together, jump down to the Putting it all Together section.</p>"},{"location":"tutorial/#next-steps","title":"Next Steps","text":"<p>If you're planning to use this library, please know it is very much in flux and I can't commit to API stability yet.</p> <p>If you are going to try to scrape using GPT, it'd probably be good to read the OpenAI API page to understand a little more about how the underlying API works.</p> <p>To see what other features are currently available, check out the Usage guide.</p> <p>You can also explore the command line interface to see how you can use this library without writing any Python.</p>"},{"location":"tutorial/#putting-it-all-together","title":"Putting it all Together","text":"<pre><code>import json\nfrom scrapeghost import SchemaScraper, CSS\n\nepisode_list_scraper = SchemaScraper(\n    '{\"url\": \"url\"}',\n    auto_split_length=1500,\n    # restrict this to GPT-3.5-Turbo to keep the cost down\n    models=[\"gpt-3.5-turbo\"],\n    extra_preprocessors=CSS(\".mw-parser-output a[class!='image link-internal']\"),\n)\n\nepisode_scraper = SchemaScraper(\n    {\n        \"title\": \"str\",\n        \"episode_number\": \"int\",\n        \"release_date\": \"YYYY-MM-DD\",\n        \"guests\": [\"str\"],\n        \"characters\": [\"str\"],\n    },\n    extra_preprocessors=CSS(\"div.page-content\"),\n)\n\nresp = episode_list_scraper(\n    \"https://comedybangbang.fandom.com/wiki/Category:Episodes\",\n)\nepisode_urls = resp.data\nprint(f\"Scraped {len(episode_urls)} episode URLs, cost {resp.total_cost}\")\n\nepisode_data = []\nfor episode_url in episode_urls:\n    print(episode_url)\n    episode_data.append(\n        episode_scraper(\n            episode_url[\"url\"],\n        ).data\n    )\n\n# scrapers have a stats() method that returns a dict of statistics across all calls\nprint(f\"Scraped {len(episode_data)} episodes, ${episode_scraper.stats()['total_cost']}\")\n\nwith open(\"episode_data.json\", \"w\") as f:\n    json.dump(episode_data, f, indent=2)\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#data-flow","title":"Data Flow","text":"<p>Since most of the work is done by the API, the job of a <code>SchemaScraper</code> is to make it easier to pass HTML and get valid output.</p> <p>If you are going to go beyond the basics, it is important to understand the data flow:</p> <ol> <li> <p>The page HTML is passed through any preprocessors.</p> <p>a. The <code>CleanHTML</code> preprocessor removes unnecessary tags and attributes.  (This is done by default.)</p> <p>b. If an <code>XPath</code> or <code>CSS</code> preprocessor is used, the results are selected and re-combined into a single HTML string.</p> <p>c. Custom preprocessors can also execute here.</p> </li> <li> <p>The HTML and schema are sent to the LLM with instructions to extract.</p> </li> <li> <p>The results are passed through any postprocessors.</p> <p>a. The <code>JSONPostprocessor</code> converts the results to JSON.  (This is done by default.) If the results are not valid JSON, a second (much smaller) request can be made to ask it to fix the JSON.</p> <p>b. Custom postprocessors can also execute here.</p> </li> </ol> <p>You can modify nearly any part of the process to suit your needs.  (See Customization for more details.)</p>"},{"location":"usage/#auto-splitting","title":"Auto-splitting","text":"<p>While the flow above covers most cases, there is one special case that is worth mentioning.</p> <p>If you set the <code>auto_split_length</code> parameter to a positive integer, the HTML will be split into multiple requests where each request aims to be no larger than <code>auto_split_length</code> tokens.</p> <p>Warning</p> <p>In list mode, a single call can make many requests. Keep an eye on the <code>max_cost</code> parameter if you're using this.</p> <p>While this seems to work well enough for long lists of similar items, the question of it is worth the time and money is up to you. Writing a bit of code is probably the better option in most cases.</p> <p>Instead of recombining the results of the <code>XPath</code> or <code>CSS</code> preprocessor, the results are instead chunked into smaller pieces (&lt;= <code>auto_split_length</code>) and sent to the API separately.</p> <p>The instructions are also modified slightly, indicating that your schema is for a list of similar items.</p>"},{"location":"usage/#customization","title":"Customization","text":"<p>To make it easier to experiment with different approaches, it is possible to customize nearly every part of the process from how the HTML is retrieved to how the results are processed.</p>"},{"location":"usage/#http-requests","title":"HTTP Requests","text":"<p>Instead of providing mechanisms to customize the HTTP request made by the library (e.g. to use caching, or make a <code>POST</code>), you can simply pass already retrieved HTML to the <code>scrape</code> method.</p> <p>This means you can use any HTTP library you want to retrieve the HTML.</p>"},{"location":"usage/#preprocessors","title":"Preprocessors","text":"<p>Preprocessors allow you to modify the HTML before it is sent to the API.</p> <p>Three preprocessors are provided:</p> <ul> <li><code>CleanHTML</code> - Cleans the HTML using <code>lxml.html.clean.Cleaner</code>.</li> <li><code>XPath</code> - Applies an XPath selector to the HTML.</li> <li><code>CSS</code> - Applies a CSS selector to the HTML.</li> </ul> <p>Note</p> <p><code>CleanHTML</code> is always applied first, as it is part of the default preprocessors list.</p> <p>You can add your own preprocessors by passing a list to the <code>extra_preprocessors</code> parameter of <code>SchemaScraper</code>.</p> <pre><code>scraper = SchemaScraper(schema, extra_preprocessors=[CSS(\"table\")])\n</code></pre> <p>It is also possible to pass preprocessors at scrape time:</p> <pre><code>scraper = SchemaScraper(schema)\nscraper.scrape(\"https://example.com\", extra_preprocessors=[CSS(\"table\")])\n</code></pre> <p>Implementing your own preprocessor is simple, just create a callable that takes a <code>lxml.html.HtmlElement</code> and returns a list of one or more <code>lxml.html.HtmlElement</code> objects.  Look at <code>preprocessors.py</code> for examples.</p>"},{"location":"usage/#altering-the-instructions-to-gpt","title":"Altering the Instructions to GPT","text":"<p>Right now you can pass additional instructions to GPT by passing a list of strings to the <code>extra_instructions</code> parameter of <code>SchemaScraper</code>.</p> <p>You can also pass <code>model_params</code> to pass additional arguments to the API.</p> <p><pre><code>schema = {\"name\": \"str\", \"committees\": [], \"bio\": \"str\"}\nscraper = SchemaScraper(\n    schema,\n    models=[\"gpt-4\"],\n    extra_instructions=[\"Put the legislator's bio in the 'bio' field. Summarize it so that it is no longer than 3 sentences.\"],\n)\nscraper.scrape(\"https://norton.house.gov/about/full-biography\").data\n</code></pre> <pre><code>{'name': 'Representative Eleanor Holmes Norton',\n'committees': [\n'House Subcommittee on Highways and Transit',\n'Committee on Oversight and Reform',\n'Committee on Transportation and Infrastructure'\n],\n'bio': 'Congresswoman Eleanor Holmes Norton has been serving as the congresswoman for the District of Columbia since 1991. She is the Chair of the House Subcommittee on Highways and Transit and serves on two committees: the Committee on Oversight and Reform and the Committee on Transportation and Infrastructure. Before her congressional service, President Jimmy Carter appointed her to serve as the first woman to chair the U.S. Equal Employment Opportunity Commission.'}\n</code></pre></p> <p>These instructions can be useful for refining the results, but they are not required.</p>"},{"location":"usage/#altering-the-api-model","title":"Altering the API / Model","text":"<p>See https://github.com/jamesturk/scrapeghost/issues/18</p>"},{"location":"usage/#postprocessors","title":"Postprocessors","text":"<p>Postprocessors take the results of the API call and modify them before returning them to the user.</p> <p>Three postprocessors are provided:</p> <ul> <li><code>JSONPostprocessor</code> - Converts the results to JSON.</li> <li><code>HallucinationChecker</code> - Checks the results for hallucinations.</li> <li><code>PydanticPostprocessor</code> - Converts the results to JSON and validates them using a <code>pydantic</code> model.</li> </ul> <p>By default, <code>JSONPostprocessor</code> and <code>HallucinationChecker</code> are enabled.</p> <p><code>HallucinationChecker</code> verifies that values in the response are present in the source HTML.  This is useful for ensuring that the results are not \"hallucinations\". This is done as a proof of concept, and to help determine how big of an issue hallucinations are for this use case.</p>"},{"location":"usage/#using-pydantic-models","title":"Using <code>pydantic</code> Models","text":"<p>If you want to validate that the returned data isn't just JSON, but data in the format you expect, you can use <code>pydantic</code> models.</p> <p><pre><code>from pydantic import BaseModel\nfrom scrapeghost import SchemaScraper, CSS\n\n\nclass CrewMember(BaseModel):\n    gender: str\n    race: str\n    alignment: str\n\n\n# passing a pydantic model to the SchemaScraper # will generate a schema from it\n# and add the PydanticPostprocessor to the postprocessors\nscrape_crewmember = SchemaScraper(schema=CrewMember)\nresult = scrape_crewmember.scrape(\n    \"https://spaceghost.fandom.com/wiki/Zorak\",\n    extra_preprocessors=[CSS(\".infobox\")],\n)\nprint(repr(result.data))\n</code></pre> <pre><code>CrewMember(gender='Male', race='Dokarian', alignment='Evil\\\\nProtagonist')\n</code></pre></p> <p>This works by converting the <code>pydantic</code> model to a schema and registering a <code>PydanticPostprocessor</code> to validate the results automatically.</p>"},{"location":"snippets/_apikey/","title":"apikey","text":""},{"location":"snippets/_apikey/#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the OpenAI API you will need an API key.  You can get one by creating an account and then creating an API key.</p> <p>It's strongly recommended that you set a usage limit on your API key to avoid accidentally running up a large bill.</p> <p>https://platform.openai.com/account/billing/limits lets you set usage limits to avoid unpleasant surprises.</p>"},{"location":"snippets/_apikey/#using-your-key","title":"Using your key","text":"<p>Once an API key is created, you can set it as an environment variable:</p> <pre><code>$ export OPENAI_API_KEY=sk-...\n</code></pre> <p>You can also set the API Key directly in Python:</p> <pre><code>import openai\n\nopenai.api_key_path = \"~/.openai-key\"\n#  - or -\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n</code></pre> <p>Be careful not to expose this key to the public by checking it into a public repository.</p>"},{"location":"snippets/_cost/","title":"cost","text":"<p>Cost per 1,000 tokens (March 19th, 2023)</p> Model Input Tokens Output Tokens GPT-3-Turbo 0.002 0.002 GPT-4 (8k) 0.03 0.06 GPT-4 (32k) 0.06 0.12 <p>Example: A 3,000 token page that returns 1,000 tokens of JSON will cost $0.008 with GPT-3-Turbo, but $0.15 with GPT-4.</p> <p>(See OpenAI pricing page for latest info.)</p>"}]}